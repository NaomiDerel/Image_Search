{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Similiarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of choosing the similarity function that determines the images that are similar to a given image.\n",
    "The implementation of the network is based on the Siamese network implementation. This class of networks is known to be more robust to class imbalance, so it fits the data on which we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         image1_path  \\\n",
      "0  (datasets/house_styles/all_images/001_d2c7428a...   \n",
      "1  (datasets/house_styles/all_images/453_d7b5d246...   \n",
      "2  (datasets/house_styles/all_images/116_32f01ef6...   \n",
      "3  (datasets/house_styles/all_images/301_b73b9663...   \n",
      "4  (datasets/house_styles/all_images/042_06b56791...   \n",
      "\n",
      "                                         image2_path  similarity  \n",
      "0  (datasets/house_styles/all_images/366_08eff319...         3.0  \n",
      "1  (datasets/house_styles/all_images/122_e44a0cb3...         0.0  \n",
      "2  (datasets/house_styles/all_images/174_55a7b3f9...         0.0  \n",
      "3  (datasets/house_styles/all_images/116_32f01ef6...         0.0  \n",
      "4  (datasets/house_styles/all_images/069_d3bedc1f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "base_path = 'active_learning_labels/'\n",
    "full_data_paths = pd.read_csv(base_path + 'round_0.csv')\n",
    "\n",
    "current_round = 1\n",
    "\n",
    "if current_round > 0:\n",
    "    for i in range(1, current_round):\n",
    "        path = base_path + 'round_' + str(i) + '.csv'\n",
    "        data = pd.read_csv(path)\n",
    "        full_data_paths = pd.concat([full_data_paths, data], ignore_index=True)\n",
    "\n",
    "data_paths = full_data_paths[['image1_path', 'image2_path', 'similarity']]\n",
    "\n",
    "print(data_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (566, 3) Eval:  (142, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "train_data = data_paths.sample(frac=0.8, random_state=42)\n",
    "eval_data = data_paths.drop(train_data.index)\n",
    "\n",
    "print(\"Train: \", train_data.shape, \"Eval: \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Computer Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CLIP model and processor from Hugging Face\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set up the image transformation pipeline\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Define the dataset class\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomApply([transforms.RandomResizedCrop(224)], p=0.3),  # 20% chance of random resized crop\n",
    "    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.3),  # 20% chance of horizontal flip\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.3),  # 20% chance of color jitter\n",
    "    transforms.ToTensor(),  # Always apply ToTensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=clip_transform, augmentations=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        self.master_path = ''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from the paths\n",
    "        image1_path = self.master_path + self.data.iloc[idx, 0].strip(\"()\")\n",
    "        image2_path = self.master_path + self.data.iloc[idx, 1].strip(\"()\")\n",
    "        \n",
    "        # Load images\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentations:\n",
    "            image1 = self.augmentations(image1)\n",
    "            image2 = self.augmentations(image2)\n",
    "\n",
    "        # # CLIP:\n",
    "        # images_features = []\n",
    "        # for img in [image1, image2]:\n",
    "        #     image_tensor = self.transform(img).unsqueeze(0)\n",
    "        #     inputs = processor(images=image_tensor, return_tensors=\"pt\")\n",
    "        #     with torch.no_grad():\n",
    "        #         image_features = model.get_image_features(**inputs)\n",
    "        #         #images_features.append(image_features.numpy().flatten())\n",
    "        #     images_features.append(image_features.squeeze())  # Ensure it's a 512-dimensional tensor\n",
    "\n",
    "        \n",
    "        # Apply CLIP transforms if provided (transforms should convert to tensor)\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)  # Apply transforms including ToTensor\n",
    "            image2 = self.transform(image2)\n",
    "\n",
    "        # Get image features using CLIP\n",
    "        images_features = []\n",
    "        for img in [image1, image2]:\n",
    "            image_tensor = img.unsqueeze(0)  # Add batch dimension for processing\n",
    "            inputs = processor(images=image_tensor, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "            images_features.append(image_features.squeeze())  # Ensure it's a 512-dimensional tensor\n",
    "\n",
    "        # Get similarity score and label (1 for similar, 0 for dissimilar)\n",
    "        similarity = self.data.iloc[idx, 2]\n",
    "        label = 0 if similarity < 3 else 1\n",
    "        \n",
    "        return image1_path, image2_path, images_features[0], images_features[1], torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageSimilarityDataset(train_data, transform=clip_transform, augmentations=None)\n",
    "eval_dataset = ImageSimilarityDataset(eval_data, transform=clip_transform, augmentations=None)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)  # Keep larger dimension here\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Using fully connected layers to process 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # First layer to reduce dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),  # Second layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)     # Output layer with 2 units (for similarity comparison)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         return self.fc1(x)\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass for both inputs\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Use fully connected layers for processing 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # Assuming CLIP outputs 512-dimensional features\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Setting up the Sequential of CNN Layers\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "            \n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to ensure the output size is consistent\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "#         # Defining the fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(1024, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n",
    "\n",
    "# # class SiameseNetwork(nn.Module):\n",
    "    \n",
    "# #     def __init__(self):\n",
    "# #         super(SiameseNetwork, self).__init__()\n",
    "# #         # Setting up the Sequential of CNN Layers\n",
    "# #         self.cnn1 = nn.Sequential(\n",
    "# #             nn.Conv2d(3, 96, kernel_size=11, stride=1),  # Adjusted for RGB input\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "# #             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "\n",
    "# #             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "# #         )\n",
    "        \n",
    "# #         # Adaptive pooling layer to ensure the output size is consistent\n",
    "# #         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "# #         # Defining the fully connected layers\n",
    "# #         self.fc1 = nn.Sequential(\n",
    "# #             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.Dropout(p=0.5),\n",
    "            \n",
    "# #             nn.Linear(1024, 128),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Linear(128, 2)\n",
    "# #         )\n",
    "        \n",
    "# #     def forward_once(self, x):\n",
    "# #         # Forward pass \n",
    "# #         x = self.cnn1(x)\n",
    "# #         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "# #         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#     #     x = self.fc1(x)\n",
    "#     #     return x\n",
    "\n",
    "#     # def forward(self, input1, input2):\n",
    "#     #     # Forward pass of input 1\n",
    "#     #     output1 = self.forward_once(input1)\n",
    "#     #     # Forward pass of input 2\n",
    "#     #     output2 = self.forward_once(input2)\n",
    "#     #     return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallSiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SmallSiameseNetwork, self).__init__()\n",
    "#         # Setting up a smaller CNN\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=1),  # Fewer filters, smaller kernel size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Max pooling with smaller stride to reduce spatial dimensions\n",
    "#             nn.Dropout2d(p=0.2),  # Reduced dropout rate\n",
    "            \n",
    "#             nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),  # Fewer filters\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Smaller max-pooling\n",
    "#             nn.Dropout2d(p=0.2),\n",
    "\n",
    "#             # nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.MaxPool2d(2, stride=2),\n",
    "#             # nn.Dropout2d(p=0.2),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to standardize output size (reduce to 3x3)\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))  # Smaller output size (3x3)\n",
    "        \n",
    "#         # Defining smaller fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(64 * 3 * 3, 128),  # Reduced size based on new feature map size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.4),  # Keep some dropout for regularization\n",
    "            \n",
    "#             nn.Linear(128, 64),  # Smaller fully connected layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(64, 2)  # Output layer remains the same\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        ### Binary:\n",
    "        # euclidian distance\n",
    "        # diff = x0 - x1\n",
    "        # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        # if torch.any(dist_sq < 0):\n",
    "        #     print('the value of dist_sq is negative: ' + dist_sq)\n",
    "        # # dist = torch.sqrt(torch.abs(dist_sq))\n",
    "        # dist = torch.sqrt(dist_sq + 1e-6)\n",
    "        # mdist = self.margin - dist\n",
    "        # dist = torch.clamp(mdist, min=0.0)\n",
    "        # loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        # loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "\n",
    "        label = y #binary?\n",
    "        euclidean_distance = nn.functional.pairwise_distance(x0, x1)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) + # similar\n",
    "                                (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)) # dissimilar\n",
    "        return loss_contrastive\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_network(model, train_loader, eval_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    max_acc = 0\n",
    "    min_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        \n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in tqdm(enumerate(train_loader)):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            dist = F.pairwise_distance(output1, output2)\n",
    "            predicted = (dist < 1.0).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Print statistics\n",
    "            # if (i + 1) % 10 == 0:  # Print every 5 batches\n",
    "            #     print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\")\n",
    "            #     running_loss = 0.0\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {correct / len(train_loader.dataset):.2f}\")\n",
    "\n",
    "        if running_loss < min_loss:\n",
    "            min_loss = running_loss\n",
    "            best_model = model.state_dict()\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        # if (epoch + 1) % 5 == 0:\n",
    "        #     model.eval()\n",
    "        #     correct = 0\n",
    "        #     with torch.no_grad():\n",
    "        #         for img1_path, img2_path, img1, img2, labels in eval_loader:\n",
    "        #             img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        #             output1, output2 = model(img1, img2)\n",
    "        #             dist = F.pairwise_distance(output1, output2)\n",
    "        #             predicted = (dist < 1.0).float()\n",
    "        #             correct += (predicted == labels).sum().item()\n",
    "        #     eval_acc = correct / len(eval_loader.dataset)\n",
    "        #     print(f\"Evaluation Accuracy: {eval_acc:.2f}\")\n",
    "\n",
    "        #     # Save the model with the best evaluation accuracy\n",
    "        #     if eval_acc >= max_acc:\n",
    "        #         max_acc = eval_acc\n",
    "        #         best_model = model.state_dict()\n",
    "        #         print(\"Best model updated\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.01)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "71it [01:29,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2969, Train Accuracy: 0.59\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.2281, Train Accuracy: 0.62\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.2199, Train Accuracy: 0.64\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.1662, Train Accuracy: 0.58\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.1605, Train Accuracy: 0.59\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.2039, Train Accuracy: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.4349, Train Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 1.8579, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.8381, Train Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2289, Train Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.1513, Train Accuracy: 0.61\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.1114, Train Accuracy: 0.61\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.1106, Train Accuracy: 0.69\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.1076, Train Accuracy: 0.68\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.1112, Train Accuracy: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.0853, Train Accuracy: 0.66\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.1005, Train Accuracy: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.0923, Train Accuracy: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.0793, Train Accuracy: 0.68\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.0645, Train Accuracy: 0.71\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:24,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.0801, Train Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.0871, Train Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:24,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.0628, Train Accuracy: 0.72\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.0569, Train Accuracy: 0.75\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.0523, Train Accuracy: 0.74\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.0553, Train Accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.0675, Train Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:24,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.0464, Train Accuracy: 0.73\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.0424, Train Accuracy: 0.78\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.0434, Train Accuracy: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.0447, Train Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.0569, Train Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.1320, Train Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.1000, Train Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.0614, Train Accuracy: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.0581, Train Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.0465, Train Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.0540, Train Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.0538, Train Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.0450, Train Accuracy: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:24,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 0.0972, Train Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 3.7945, Train Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 28.9140, Train Accuracy: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 4.0878, Train Accuracy: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:25,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Loss: 0.2258, Train Accuracy: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Loss: 0.0931, Train Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Loss: 0.0733, Train Accuracy: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Loss: 0.0625, Train Accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Loss: 0.0534, Train Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 0.0491, Train Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Loss: 0.0456, Train Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Loss: 0.0452, Train Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Loss: 0.0411, Train Accuracy: 0.80\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Loss: 0.0396, Train Accuracy: 0.80\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Loss: 0.0378, Train Accuracy: 0.79\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Loss: 0.0379, Train Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Loss: 0.0342, Train Accuracy: 0.80\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Loss: 0.0311, Train Accuracy: 0.80\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Loss: 0.0314, Train Accuracy: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:24,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 0.0307, Train Accuracy: 0.84\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Loss: 0.0309, Train Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Loss: 0.0269, Train Accuracy: 0.84\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Loss: 0.0272, Train Accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Loss: 0.0267, Train Accuracy: 0.83\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Loss: 0.0261, Train Accuracy: 0.85\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Loss: 0.0328, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Loss: 0.0250, Train Accuracy: 0.86\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Loss: 0.0281, Train Accuracy: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Loss: 0.0252, Train Accuracy: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Loss: 0.0272, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Loss: 0.0236, Train Accuracy: 0.86\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Loss: 0.0264, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Loss: 0.0231, Train Accuracy: 0.87\n",
      "Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Loss: 0.0277, Train Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Loss: 0.0231, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Loss: 0.0274, Train Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Loss: 0.0413, Train Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Loss: 0.0402, Train Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Loss: 0.0480, Train Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Loss: 0.0333, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Loss: 0.0391, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Loss: 0.0487, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Loss: 0.0424, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:26,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Loss: 0.0595, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Loss: 0.0377, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Loss: 0.0369, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Loss: 0.0266, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Loss: 0.0243, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Loss: 0.1481, Train Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Loss: 0.0651, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Loss: 0.0259, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Loss: 0.0444, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:29,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Loss: 0.0317, Train Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:27,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Loss: 0.0269, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Loss: 0.0294, Train Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Loss: 0.0241, Train Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Loss: 0.0344, Train Accuracy: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:30,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Loss: 0.0646, Train Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Loss: 0.0922, Train Accuracy: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0346, Train Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_siamese_network(siamese_net, train_loader, eval_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(siamese_net.state_dict(), 'siamese_net_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182634/3788960927.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  siamese_net.load_state_dict(torch.load('siamese_net_4.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "siamese_net.load_state_dict(torch.load('siamese_net_4.pth'))\n",
    "trained_model = siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.6972\n",
      "Test F1 Score: 0.0444\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_siamese_network(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in enumerate(test_loader):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            # Calculate the euclidean distance between the outputs\n",
    "            dist = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Get predictions\n",
    "            predicted = (dist < 1.0).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "            # print(f\"Dist: {dist}, Predicted: {predicted}, Actual: {labels}\")\n",
    "\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "\n",
    "    # accuracy:\n",
    "    accuracy = correct / total\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # move to cpu:\n",
    "    all_labels = [label.cpu().numpy() for label in all_labels]\n",
    "    all_predictions = [prediction.cpu().numpy() for prediction in all_predictions]\n",
    "\n",
    "    # f1 score:\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # confusion matrix:\n",
    "    matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "test_siamese_network(trained_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8587\n",
      "Test F1 Score: 0.5455\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAHACAYAAAA7urvtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6HUlEQVR4nO3df3zOdf////uxzQ6MbSbbTMivsPKrVRwVEjZScVq/lXGKD43KIu2d36qV6lRKdPbDdCL95MxO6lxk6jRhKImdTQrZMUrb2sqxX8f3DxfHt+NEjmOOOfC8Xbu8LhfH8/U8nsfjOE+1xx6P1/P1sjidTqcAAIBxAvwdAAAA8A+SAAAADEUSAACAoUgCAAAwFEkAAACGIgkAAMBQJAEAABiKJAAAAEORBAAAYKggfwdQE+p0GevvEIAatyXjaX+HANS4y5qE1Oj6vvx58fu2l3y21tlyQSYBAAB4xGJ2Qdzsbw8AgMGoBAAAzGWx+DsCvyIJAACYi3YAAAAwEZUAAIC5aAcAAGAo2gEAAMBEVAIAAOaiHQAAgKFoBwAAABNRCQAAmIt2AAAAhqIdAAAATEQlAABgLtoBAAAYinYAAAAwEZUAAIC5aAcAAGAo2gEAAMBEVAIAAOYyvBJAEgAAMFeA2dcEmJ0CAQBgMCoBAABz0Q4AAMBQhm8RNDsFAgDAYFQCAADmoh0AAIChaAcAAAB/eeqpp2SxWPTQQw+5xo4ePark5GQ1bNhQ9erVU2JiogoKCtzet2/fPg0YMEB169ZVZGSkJk6cqIqKCq8+myQAAGAuS4DvjmrYvHmzXnnlFXXs2NFtfPz48Vq5cqXeffddZWVl6eDBgxo8eLDrfGVlpQYMGKCysjJt2LBBixYtUnp6uqZOnerV55MEAADMZbH47vBSSUmJhgwZoldffVUNGjRwjRcVFen111/X3/72N91www2Ki4vTwoULtWHDBm3cuFGS9O9//1vffPONFi9erM6dO6t///6aNWuW5s2bp7KyMo9jIAkAAMAHHA6HiouL3Q6Hw3HK+cnJyRowYID69OnjNp6Tk6Py8nK38Xbt2qlZs2bKzs6WJGVnZ6tDhw6KiopyzUlISFBxcbF27tzpccwkAQAAc/mwHZCWlqawsDC3Iy0t7aQfu2zZMm3duvWk5+12u4KDgxUeHu42HhUVJbvd7przxwTg+Pnj5zzF7gAAgLl8uDsgNTVVKSkpbmNWq/WEefv379eDDz6ozMxM1a5d22efXx1UAgAA8AGr1arQ0FC342RJQE5Ojg4dOqQrrrhCQUFBCgoKUlZWlubOnaugoCBFRUWprKxMhYWFbu8rKChQdHS0JCk6OvqE3QLHXx+f4wmSAACAufywO6B3797asWOHtm/f7jquvPJKDRkyxPXnWrVqac2aNa735Obmat++fbLZbJIkm82mHTt26NChQ645mZmZCg0NVWxsrMex0A4AAJjLDzcLql+/vi6//HK3sZCQEDVs2NA1PmLECKWkpCgiIkKhoaEaN26cbDabunXrJkmKj49XbGys7r33Xs2ePVt2u12TJ09WcnLySasPp0ISAADAOWbOnDkKCAhQYmKiHA6HEhIS9PLLL7vOBwYGKiMjQ2PGjJHNZlNISIiSkpI0c+ZMrz7H4nQ6nb4O3t/qdBnr7xCAGrcl42l/hwDUuMuahNTo+nVueslna/2ecf797KESAAAwl+EPEDL72wMAYDAqAQAAcxn+FEGSAACAuWgHAAAAE1EJAACYi3YAAACGoh0AAABMRCUAAGAu2gEAAJjJYngSQDsAAABDUQkAABjL9EoASQAAwFxm5wC0AwAAMBWVAACAsWgHAABgKNOTANoBAAAYikoAAMBYplcCSAIAAMYyPQmgHQAAgKGoBAAAzGV2IYAkAABgLtoBAADASFQCAADGMr0SQBIAADCW6UkA7QAAAAxFJQAAYCzTKwEkAQAAc5mdA9AOAADAVFQCAADGoh0AAIChTE8CaAcAAGAoKgEAAGNRCQAAwFQWHx5emD9/vjp27KjQ0FCFhobKZrNp9erVrvPXX3+9LBaL2zF69Gi3Nfbt26cBAwaobt26ioyM1MSJE1VRUeFVHFQCAAA4yy6++GI99dRTatOmjZxOpxYtWqSBAwdq27ZtuuyyyyRJI0eO1MyZM13vqVu3ruvPlZWVGjBggKKjo7Vhwwbl5+dr6NChqlWrlp588kmP4yAJAAAYy1/tgJtvvtnt9RNPPKH58+dr48aNriSgbt26io6OPun7//3vf+ubb77RJ598oqioKHXu3FmzZs3SpEmTNH36dAUHB3sUB+0AAICx/rfkfiaHw+FQcXGx2+FwOE4bQ2VlpZYtW6bS0lLZbDbX+JIlS3TRRRfp8ssvV2pqqn777TfXuezsbHXo0EFRUVGusYSEBBUXF2vnzp0ef3+SAAAAfCAtLU1hYWFuR1pa2inn79ixQ/Xq1ZPVatXo0aO1fPlyxcbGSpLuvvtuLV68WJ9++qlSU1P1j3/8Q/fcc4/rvXa73S0BkOR6bbfbPY6ZdgAAwFi+bAekpqYqJSXFbcxqtZ5yftu2bbV9+3YVFRXpvffeU1JSkrKyshQbG6tRo0a55nXo0EGNGzdW7969tWfPHrVq1cpnMZMEAACM5cskwGq1/ukP/f8VHBys1q1bS5Li4uK0efNmvfDCC3rllVdOmNu1a1dJUl5enlq1aqXo6Ght2rTJbU5BQYEknfI6gpOhHQAAwDmgqqrqlNcQbN++XZLUuHFjSZLNZtOOHTt06NAh15zMzEyFhoa6WgqeoBIAADCXn+4VlJqaqv79+6tZs2b69ddftXTpUq1bt04ff/yx9uzZo6VLl+rGG29Uw4YN9dVXX2n8+PHq0aOHOnbsKEmKj49XbGys7r33Xs2ePVt2u12TJ09WcnKyV9UIkgAAgLH8tUXw0KFDGjp0qPLz8xUWFqaOHTvq448/Vt++fbV//3598sknev7551VaWqqmTZsqMTFRkydPdr0/MDBQGRkZGjNmjGw2m0JCQpSUlOR2XwFPkAQAAHCWvf7666c817RpU2VlZZ12jebNm2vVqlVnFAdJAADAWKY/O4AkAABgLNOTAHYHAABgKCoBAABzmV0IIAkAAJiLdgAAADASlQB4ZMLwvpr1wEC9tORTTXz2fUnSi4/dqRu6tlXjRmEq+d2hjV/u1eQX/qn/fl/gel9cbDPNemCgusQ2ldMpbfn6Bz32wgrt+O+P/voqwJ/6f3cN0OGC/BPG+w28TaMeTJX9x/1KX/C8dn+9TeXl5epy1TW6b9wjCo9o6IdocaaoBACnERfbTCMSr9VX/z3gNr5t136Nmr5YnQc/rlvunyeLxaKMl5MVEHDsX6qQOsH657xk7bf/oh73Pqvew/+mkt+O6sN5yQoK4q8ezk2z5y/W6+/923VMe2a+JOmann119PffNeORZFks0oznXtGTc99QRUW5nnzsIVVVVfk5clSHLx8lfD7iv8T4UyF1grXwyWG6f9ZbKiz+3e3cGx/8R//Zukf78o9o++4DmjFvpZo2jlDzmGO/EbVtEa2G4SGaNT9D3/5wSLu+s+uJV1Yr+qJQNWsc4Y+vA5xWWHgDNYi4yHVsyV6v6JiLdVmnOO3+ersOFxzUuEkz1LxlGzVv2UbjJs3Qnv9+ox3bNvs7dMBrJAH4U8+n3qGPPvtan36R+6fz6tYO1tBbumnvgZ90wP6LJOm/3xfop19KlDToGtUKClRtay0NG2TTru/y9cPBI2cjfOCMlJeXa/0nq3VD/4GyWCwqLy+TZFGtWsGuOcHBVlksAdq1Y5v/AkW1mV4J8Os1AT/99JPeeOMNZWdny263Szr2CMRrrrlGw4YNU6NGjfwZnvFuS4hT53ZNdd09s085Z9Rt3fXEQ4NUr65VuXvtGjDmJZVXVEqSSn5zKGHkC3rnb6OUOrKfJClv3yHdkjxPlZWUTnHu2/SfT1Va8qtuSLhFknRpbEfVrlNHb/79Bd1z31g5ndI/Xp2rqqpK/XLkJz9Hi2o5P392+4zfKgGbN2/WpZdeqrlz5yosLEw9evRQjx49FBYWprlz56pdu3basmXLaddxOBwqLi52O5xVlWfhG1zYLo4K1zMTEzX8sXQ5yipOOW/Z6s3qdtdT6jNijr7dd1iLn/6rrMHHcsva1lpaMG2Isr/8Tj2HPqsbhv9N3+zJ1wdzx6i2tdbZ+ipAta1ZtUJXXH2NIi469gtJWHgDTZj6tLZkf6a7B1yne27uod9KflXLNu0UYKGwivOP3yoB48aN02233aYFCxacUEZxOp0aPXq0xo0bp+zs7D9dJy0tTTNmzHAbC4y6SrUaX+3zmE3SpX0zRTUMVfbSSa6xoKBAXXdFK42+o4fCuj6kqiqnikuOqrjkqPbsO6xNX32v/PWzNfCGTnrnoxzd0f9KNYuJUM+k5+R0OiVJSanpyl8/Wzdf31Hvfpzjr68HnNYh+0F9tXWTHpnxrNt456tsmr/kQxUX/aLAwCCF1Kuvvyb2VVTjJn6KFGfifC3j+4rfkoAvv/xS6enpJ/0/wGKxaPz48erSpctp10lNTVVKSorbWGT3SaeYDU99uilXcbc+4Tb29xn3KHdvgZ5Lz1RVlfOE91gsFllkUXCtY3+t6tYOVlWV05UASFKV0ymnUwow/F88nPvWfvShQsMjFNftupOeDw1rIEnasXWTigqP6Kprep7N8OAjJAF+Eh0drU2bNqldu3YnPb9p0yZFRUWddh2r1Sqr1eo2ZgkI9EmMJiv5zaFv9rjvlS79vUxHikr1zZ58XdKkoW5NiNOa7F366ZcSNYkK18PD4/W7o1wff75TkrRm4249+dAgPZ96u+Yvy1KAxaIJw+NVUVmprC3/9cfXAjxSVVWltR99qF7xNykw0P0/k2tW/1MXN2+hsLAGyv3mK70+71nddOsQNWl2iX+CBc6A35KACRMmaNSoUcrJyVHv3r1dP/ALCgq0Zs0avfrqq3r22WdPswr8xVFWoWu7tNLYu69Xg9C6OvTzr/p8a556DXtOh38pkXRsd0Dig6/osf/XX+sWPayqKqe+3H1AA5Nflv2nYj9/A+DUvsr5Qj8dsqt3/4EnnDu4/wctee0llfxapEbRMbp1yAjdfOsQP0QJXzC8ECCL84+12rPs7bff1pw5c5STk6PKymMX8wUGBiouLk4pKSm6/fbbq7VunS5jfRkmcE7akvG0v0MAatxlTUJqdP02Ez/y2VrfPtPPZ2udLX7dInjHHXfojjvuUHl5uX766dj2mosuuki1anHlOAAANe2ceHZArVq11LhxY3+HAQAwjOntgHMiCQAAwB9M3x3A3S0AADAUlQAAgLEMLwSQBAAAzHX80eemoh0AAIChqAQAAIxlejuASgAAAIaiEgAAMJbpWwRJAgAAxjI8B6AdAACAqagEAACMRTsAAABDmZ4E0A4AAMBQVAIAAMYyvBBAJQAAYC6LxeKzwxvz589Xx44dFRoaqtDQUNlsNq1evdp1/ujRo0pOTlbDhg1Vr149JSYmqqCgwG2Nffv2acCAAapbt64iIyM1ceJEVVRUeBUHSQAAAGfZxRdfrKeeeko5OTnasmWLbrjhBg0cOFA7d+6UJI0fP14rV67Uu+++q6ysLB08eFCDBw92vb+yslIDBgxQWVmZNmzYoEWLFik9PV1Tp071Kg6L0+l0+vSbnQPqdBnr7xCAGrcl42l/hwDUuMuahNTo+lfMXOuztbZOveGM3h8REaFnnnlGt956qxo1aqSlS5fq1ltvlSTt3r1b7du3V3Z2trp166bVq1frpptu0sGDBxUVFSVJWrBggSZNmqTDhw8rODjYo8+kEgAAMJYv2wEOh0PFxcVuh8PhOG0MlZWVWrZsmUpLS2Wz2ZSTk6Py8nL16dPHNaddu3Zq1qyZsrOzJUnZ2dnq0KGDKwGQpISEBBUXF7uqCZ4gCQAAwAfS0tIUFhbmdqSlpZ1y/o4dO1SvXj1ZrVaNHj1ay5cvV2xsrOx2u4KDgxUeHu42PyoqSna7XZJkt9vdEoDj54+f8xS7AwAAxvLl7oDU1FSlpKS4jVmt1lPOb9u2rbZv366ioiK99957SkpKUlZWlu8C8gBJAADAWL68WZDVav3TH/r/Kzg4WK1bt5YkxcXFafPmzXrhhRd0xx13qKysTIWFhW7VgIKCAkVHR0uSoqOjtWnTJrf1ju8eOD7HE7QDAAA4B1RVVcnhcCguLk61atXSmjVrXOdyc3O1b98+2Ww2SZLNZtOOHTt06NAh15zMzEyFhoYqNjbW48+kEgAAMJa/bhaUmpqq/v37q1mzZvr111+1dOlSrVu3Th9//LHCwsI0YsQIpaSkKCIiQqGhoRo3bpxsNpu6desmSYqPj1dsbKzuvfdezZ49W3a7XZMnT1ZycrJX1QiSAACAsfz17IBDhw5p6NChys/PV1hYmDp27KiPP/5Yffv2lSTNmTNHAQEBSkxMlMPhUEJCgl5++WXX+wMDA5WRkaExY8bIZrMpJCRESUlJmjlzpldxcJ8A4DzFfQJggpq+T0DXNN9diPdFak+frXW2UAkAABjL9GcHkAQAAIzFo4QBAICRqAQAAIxleCGAJAAAYC7aAQAAwEhUAgAAxjK8EEASAAAwF+0AAABgJCoBAABjmV4JIAkAABjL8ByAdgAAAKaiEgAAMBbtAAAADGV4DkA7AAAAU1EJAAAYi3YAAACGMjwHoB0AAICpqAQAAIwVYHgpgCQAAGAsw3MA2gEAAJiKSgAAwFjsDgAAwFABZucAtAMAADAVlQAAgLFoBwAAYCjDcwDaAQAAmIpKAADAWBaZXQrwSRJQWFio8PBwXywFAMBZw+4ALz399NN6++23Xa9vv/12NWzYUE2aNNGXX37p0+AAAEDN8ToJWLBggZo2bSpJyszMVGZmplavXq3+/ftr4sSJPg8QAICaYrFYfHacj7xuB9jtdlcSkJGRodtvv13x8fG65JJL1LVrV58HCABATTlPf3b7jNeVgAYNGmj//v2SpI8++kh9+vSRJDmdTlVWVvo2OgAAUGO8TgIGDx6su+++W3379tXPP/+s/v37S5K2bdum1q1b+zxAAABqSoDF4rPDG2lpabrqqqtUv359RUZGatCgQcrNzXWbc/3115/Qchg9erTbnH379mnAgAGqW7euIiMjNXHiRFVUVHgch9ftgDlz5uiSSy7R/v37NXv2bNWrV0+SlJ+fr/vvv9/b5QAA8Bt/tQOysrKUnJysq666ShUVFfq///s/xcfH65tvvlFISIhr3siRIzVz5kzX67p167r+XFlZqQEDBig6OlobNmxQfn6+hg4dqlq1aunJJ5/0KA6L0+l0+u5rnRvqdBnr7xCAGrcl42l/hwDUuMuahJx+0hlIfCPHZ2u9/9e4ar/38OHDioyMVFZWlnr06CHpWCWgc+fOev7550/6ntWrV+umm27SwYMHFRUVJenYxfuTJk3S4cOHFRwcfNrP9agS8OGHH3r4NaRbbrnF47kAAPiTL6/qdzgccjgcbmNWq1VWq/W07y0qKpIkRUREuI0vWbJEixcvVnR0tG6++WZNmTLFVQ3Izs5Whw4dXAmAJCUkJGjMmDHauXOnunTpctrP9SgJGDRokCfTZLFYuDgQAHDe8GU7IC0tTTNmzHAbmzZtmqZPn/6n76uqqtJDDz2ka6+9Vpdffrlr/O6771bz5s0VExOjr776SpMmTVJubq4++OADScd26/0xAZDkem232z2K2aMkoKqqyqPFAAAwVWpqqlJSUtzGPKkCJCcn6+uvv9bnn3/uNj5q1CjXnzt06KDGjRurd+/e2rNnj1q1auWTmM/otsFHjx5V7dq1fRIIAABnm7dX9f8ZT0v/fzR27FhlZGRo/fr1uvjii/907vF78eTl5alVq1aKjo7Wpk2b3OYUFBRIkqKjoz36fK+3CFZWVmrWrFlq0qSJ6tWrp++++06SNGXKFL3++uveLgcAgN9YfHh4w+l0auzYsVq+fLnWrl2rFi1anPY927dvlyQ1btxYkmSz2bRjxw4dOnTINSczM1OhoaGKjY31KA6vk4AnnnhC6enpmj17ttuVh5dffrlee+01b5cDAMA4ycnJWrx4sZYuXar69evLbrfLbrfr999/lyTt2bNHs2bNUk5Ojr7//nt9+OGHGjp0qHr06KGOHTtKkuLj4xUbG6t7771XX375pT7++GNNnjxZycnJHlckvE4C3nzzTf3973/XkCFDFBgY6Brv1KmTdu/e7e1yAAD4jb+eHTB//nwVFRXp+uuvV+PGjV3H8Qf0BQcH65NPPlF8fLzatWunhx9+WImJiVq5cqVrjcDAQGVkZCgwMFA2m0333HOPhg4d6nZfgdPx+pqAH3/88aR3BqyqqlJ5ebm3ywEA4Df+epTw6W7R07RpU2VlZZ12nebNm2vVqlXVjsPrSkBsbKw+++yzE8bfe+89j/YkAgCAc4PXlYCpU6cqKSlJP/74o6qqqvTBBx8oNzdXb775pjIyMmoiRgAAasT5+ghgX/G6EjBw4ECtXLlSn3zyiUJCQjR16lTt2rVLK1euVN++fWsiRgAAaoTF4rvjfFSt+wR0795dmZmZvo4FAACcRdW+WdCWLVu0a9cuSceuE4iLq/6DEwAA8AfT2wFeJwEHDhzQXXfdpf/85z8KDw+XJBUWFuqaa67RsmXLTnvHIwAAzhX+2h1wrvD6moD77rtP5eXl2rVrl44cOaIjR45o165dqqqq0n333VcTMQIAgBrgdSUgKytLGzZsUNu2bV1jbdu21Ysvvqju3bv7NDgAAGoS7QAvNW3a9KQ3BaqsrFRMTIxPggIA4GwwOwWoRjvgmWee0bhx47RlyxbX2JYtW/Tggw/q2Wef9WlwAACg5nhUCWjQoIFbyaS0tFRdu3ZVUNCxt1dUVCgoKEh//etfNWjQoBoJFAAAX/Plo4TPRx4lAc8//3wNhwEAwNlneA7gWRKQlJRU03EAAICzrNo3C5Kko0ePqqyszG0sNDT0jAICAOBsMX13gNcXBpaWlmrs2LGKjIxUSEiIGjRo4HYAAHC+MP3ZAV4nAY888ojWrl2r+fPny2q16rXXXtOMGTMUExOjN998syZiBAAANcDrdsDKlSv15ptv6vrrr9fw4cPVvXt3tW7dWs2bN9eSJUs0ZMiQmogTAACfM313gNeVgCNHjqhly5aSjvX/jxw5Ikm67rrrtH79et9GBwBADaId4KWWLVtq7969kqR27drpnXfekXSsQnD8gUIAAODc53USMHz4cH355ZeSpEcffVTz5s1T7dq1NX78eE2cONHnAQIAUFMsFovPjvORxel0Os9kgR9++EE5OTlq3bq1Onbs6Ku4zsjRCn9HANS8w8UOf4cA1LimEdYaXX/c8l0+W+vFv7T32VpnyxndJ0CSmjdvrubNm/siFgAAcBZ5lATMnTvX4wUfeOCBagcDAMDZdL6W8X3FoyRgzpw5Hi1msVhIAgAA540As3MAz5KA47sBAADAheOMrwkAAOB8RSUAAABDmX5NgNf3CQAAABcGKgEAAGPRDgAAwFCGdwOq1w747LPPdM8998hms+nHH3+UJP3jH//Q559/7tPgAABAzfE6CXj//feVkJCgOnXqaNu2bXI4jt26tKioSE8++aTPAwQAoKYEWCw+O85HXicBjz/+uBYsWKBXX31VtWrVco1fe+212rp1q0+DAwCgJgX48DgfeR13bm6uevToccJ4WFiYCgsLfRETAAAXtLS0NF111VWqX7++IiMjNWjQIOXm5rrNOXr0qJKTk9WwYUPVq1dPiYmJKigocJuzb98+DRgwQHXr1lVkZKQmTpyoigrPn6LndRIQHR2tvLy8E8Y///xztWzZ0tvlAADwG4vFd4c3srKylJycrI0bNyozM1Pl5eWKj49XaWmpa8748eO1cuVKvfvuu8rKytLBgwc1ePBg1/nKykoNGDBAZWVl2rBhgxYtWqT09HRNnTrV8+/v7aOE09LStHjxYr3xxhvq27evVq1apR9++EHjx4/XlClTNG7cOG+WqxE8Shgm4FHCMEFNP0p4ykff+mytWf3aVPu9hw8fVmRkpLKystSjRw8VFRWpUaNGWrp0qW699VZJ0u7du9W+fXtlZ2erW7duWr16tW666SYdPHhQUVFRkqQFCxZo0qRJOnz4sIKDg0/7uV5vEXz00UdVVVWl3r1767ffflOPHj1ktVo1YcKEcyIBAADAHxwOh+ti+eOsVqus1tMnMkVFRZKkiIgISVJOTo7Ky8vVp08f15x27dqpWbNmriQgOztbHTp0cCUAkpSQkKAxY8Zo586d6tKly2k/1+t2gMVi0WOPPaYjR47o66+/1saNG3X48GHNmjXL26UAAPArX7YD0tLSFBYW5nakpaWdNoaqqio99NBDuvbaa3X55ZdLkux2u4KDgxUeHu42NyoqSna73TXnjwnA8fPHz3mi2jcLCg4OVmxsbHXfDgCA3/nyjoGpqalKSUlxG/OkCpCcnKyvv/7aL/fa8ToJ6NWr158+cGHt2rVnFBAAAOcjT0v/fzR27FhlZGRo/fr1uvjii13j0dHRKisrU2FhoVs1oKCgQNHR0a45mzZtclvv+O6B43NOx+t2QOfOndWpUyfXERsbq7KyMm3dulUdOnTwdjkAAPzGXzcLcjqdGjt2rJYvX661a9eqRYsWbufj4uJUq1YtrVmzxjWWm5urffv2yWazSZJsNpt27NihQ4cOueZkZmYqNDTU40q915WAOXPmnHR8+vTpKikp8XY5AAD8xl83+ktOTtbSpUv1z3/+U/Xr13f18MPCwlSnTh2FhYVpxIgRSklJUUREhEJDQzVu3DjZbDZ169ZNkhQfH6/Y2Fjde++9mj17tux2uyZPnqzk5GSPKxJebxE8lby8PF199dU6cuSIL5Y7I2wRhAnYIggT1PQWwVmfnHjfm+qa0qe1x3NP1VZfuHChhg0bJunYzYIefvhhvfXWW3I4HEpISNDLL7/sVur/4YcfNGbMGK1bt04hISFKSkrSU089paAgz37H99lTBLOzs1W7dm1fLQcAQI3z16OEPfn9u3bt2po3b57mzZt3yjnNmzfXqlWrqh2H10nAH+9WJB37Ivn5+dqyZYumTJlS7UAAADjbLDo/H/zjK14nAWFhYW6vAwIC1LZtW82cOVPx8fE+CwwAANQsr5KAyspKDR8+XB06dFCDBg1qKiYAAM4Kf7UDzhVebREMDAxUfHw8TwsEAFwQAiy+O85HXt8n4PLLL9d3331XE7EAAICzyOsk4PHHH9eECROUkZGh/Px8FRcXux0AAJwvLBaLz47zkcfXBMycOVMPP/ywbrzxRknSLbfc4valnU6nLBaLKisrfR8lAAA14Hwt4/uKx0nAjBkzNHr0aH366ac1GQ8AADhLPE4Cjt/YoGfPnjUWDAAAZ9N5WsX3Ga+2CJ6vPQ8AAE7G2wf/XGi8SgIuvfTS0yYC58KzAwAAwOl5lQTMmDHjhDsGAgBwvuLCQC/ceeedioyMrKlYAAA4qwzvBnh+nwCuBwAA4MLi9e4AAAAuFAE8RdAzVVVVNRkHAABnnelFbq9vGwwAAC4MXl0YCADAhYTdAQAAGMr0mwXRDgAAwFBUAgAAxjK8EEASAAAwF+0AAABgJCoBAABjGV4IIAkAAJjL9HK46d8fAABjUQkAABjL9IfjkQQAAIxldgpAOwAAAGNRCQAAGMv0+wSQBAAAjGV2CkA7AAAAY1EJAAAYy/BuAJUAAIC5LBaLzw5vrF+/XjfffLNiYmJksVi0YsUKt/PDhg07Yf1+/fq5zTly5IiGDBmi0NBQhYeHa8SIESopKfEqDpIAAADOstLSUnXq1Enz5s075Zx+/fopPz/fdbz11ltu54cMGaKdO3cqMzNTGRkZWr9+vUaNGuVVHLQDAADG8tdvwv3791f//v3/dI7ValV0dPRJz+3atUsfffSRNm/erCuvvFKS9OKLL+rGG2/Us88+q5iYGI/ioBIAADCWL9sBDodDxcXFbofD4ah2bOvWrVNkZKTatm2rMWPG6Oeff3ady87OVnh4uCsBkKQ+ffooICBAX3zxhcefQRIAAIAPpKWlKSwszO1IS0ur1lr9+vXTm2++qTVr1ujpp59WVlaW+vfvr8rKSkmS3W5XZGSk23uCgoIUEREhu93u8efQDgAAGMuXmwNSU1OVkpLiNma1Wqu11p133un6c4cOHdSxY0e1atVK69atU+/evc8ozj8iCQAAGMuXDxCyWq3V/qF/Oi1bttRFF12kvLw89e7dW9HR0Tp06JDbnIqKCh05cuSU1xGcDO0AAADOcQcOHNDPP/+sxo0bS5JsNpsKCwuVk5PjmrN27VpVVVWpa9euHq9LJQAAYCx//SZcUlKivLw81+u9e/dq+/btioiIUEREhGbMmKHExERFR0drz549euSRR9S6dWslJCRIktq3b69+/fpp5MiRWrBggcrLyzV27FjdeeedHu8MkCSL0+l0+vzb+dnRCn9HANS8w8XVv+oYOF80jaiZ8vpxy7/y/CK60/lLR8/L8OvWrVOvXr1OGE9KStL8+fM1aNAgbdu2TYWFhYqJiVF8fLxmzZqlqKgo19wjR45o7NixWrlypQICApSYmKi5c+eqXr16HsdBEgCcp0gCYIILNQk4V9AOAAAYy/BHB5AEAADMxQOEAACAkagEAACMFWB4Q4AkAABgLNoBAADASFQCAADGstAOAADATLQDAACAkagEAACMxe4AAAAMRTsAAAAYiUoAAMBYplcCSAIAAMYyfYsg7QAAAAxFJQAAYKwAswsBJAEAAHPRDgAAAEaiEgAAMBa7AwAAMBTtAAAAYCQqAQAAY7E7AAAAQ9EOAKopZ8tmjbt/tPpcf506XdZWa9d84u+QAJ96683X1cfWUS/Pedo1duTnn/TUjP/TbQN66aZeV2t00u1a/2mmH6MEqo8kANX2+++/qW3btkqdPM3foQA+t/ubr/WvFe+qZetL3cafnvmY9v/wvWbNnqu/L/5A113fR49Pnqhvc3f5KVKcCYvFd8f5iCQA1XZd954a++B49e7T19+hAD71+2+/KW16qsY/Ol316oe6ndu5Y7sG3XaX2l3WQTFNLtY9w0cppF59fZv7jZ+ixZmw+PA4H5EEAMD/mPvsE+p6TXfFXd3thHOXdeisdZ98rOKiIlVVVenTzNUqL3OoU5er/BApcGbO6QsD9+/fr2nTpumNN9445RyHwyGHw+E25gy0ymq11nR4AC5An2au1re5u/TyG2+d9PyUx5/RrCmPaHC/7goMDJK1dm1Nf+p5NWna7CxHCl8IOF/r+D5yTlcCjhw5okWLFv3pnLS0NIWFhbkdzzyddpYiBHAhOVRg17w5T+v/Zjyl4FP8IrHw7/NU+muxZs/9u15e+JZuvetezZo8Ud/l/fcsRwtfML0d4NdKwIcffvin57/77rvTrpGamqqUlBS3MWcgVQAA3vt29zcq/OWIRg+7wzVWVVmpHdtztOL9ZUpf9qH++d5bem3JB7qkZWtJUqs2bbVj+1Z9+P7bemjSFH+FDlSLX5OAQYMGyWKxyOl0nnKO5TSlGqv1xNL/0QqfhAfAMF2u7KpXF7/vNvbME1PVrHkL3XHPcB09+rskyRLgXkQNCAxUlbPqrMUJHzpff4X3Eb+2Axo3bqwPPvhAVVVVJz22bt3qz/BwGr+Vlmr3rl3avevY1qgfDxzQ7l27lH/woJ8jA6qnbkiIWrRq43bUrl1HoaFhatGqjZpd0kJNLm6m55+eqd07d+jggf16d+kibd2UrWt73ODv8FENFh/+cz7yayUgLi5OOTk5Gjhw4EnPn65KAP/aufNr3Td8qOv1s7OPXYtxy8C/aNaTT/krLKDGBAXV0hN/m6fXXn5ekyeO09Hff1PMxc30yJTH1fWa7v4OD/CaxenHn7KfffaZSktL1a9fv5OeLy0t1ZYtW9SzZ0+v1qUdABMcLnacfhJwnmsaUbPXeG36rshna13dMszjuevXr9czzzyjnJwc5efna/ny5Ro0aJDrvNPp1LRp0/Tqq6+qsLBQ1157rebPn682bdq45hw5ckTjxo3TypUrFRAQoMTERL3wwguqV6+ex3H4tR3QvXv3UyYAkhQSEuJ1AgAAgKf8tTugtLRUnTp10rx58056fvbs2Zo7d64WLFigL774QiEhIUpISNDRo0ddc4YMGaKdO3cqMzNTGRkZWr9+vUaNGuVVHH6tBNQUKgEwAZUAmKCmKwGbfVgJuMqLSsAfWSwWt0qA0+lUTEyMHn74YU2YMEGSVFRUpKioKKWnp+vOO+/Url27FBsbq82bN+vKK6+UJH300Ue68cYbdeDAAcXExHj02ef0fQIAAKhR5+CNAvbu3Su73a4+ffq4xsLCwtS1a1dlZ2dLkrKzsxUeHu5KACSpT58+CggI0BdffOHxZ53TdwwEAKAm+fKq/pPdwfZk29hPx263S5KioqLcxqOiolzn7Ha7IiMj3c4HBQUpIiLCNccTVAIAAPCBk93BNi3t3L6DLZUAAICxfPnogJPdwbY6z7GJjo6WJBUUFKhx48au8YKCAnXu3Nk159ChQ27vq6io0JEjR1zv9wSVAAAAfMBqtSo0NNTtqE4S0KJFC0VHR2vNmjWuseLiYn3xxRey2WySJJvNpsLCQuXk5LjmrF27VlVVVeratavHn0UlAABgLH/d56+kpER5eXmu13v37tX27dsVERGhZs2a6aGHHtLjjz+uNm3aqEWLFpoyZYpiYmJcOwjat2+vfv36aeTIkVqwYIHKy8s1duxY3XnnnR7vDJBIAgAAJvNTFrBlyxb16tXL9fp4GyEpKUnp6el65JFHVFpaqlGjRqmwsFDXXXedPvroI9WuXdv1niVLlmjs2LHq3bu362ZBc+fO9SoO7hMAnKe4TwBMUNP3Cdj6Q7HP1rqieajP1jpbqAQAAIx1vj74x1dIAgAAxvLl7oDzEbsDAAAwFJUAAICxDC8EkAQAAAxmeBZAOwAAAENRCQAAGIvdAQAAGIrdAQAAwEhUAgAAxjK8EEASAAAwmOFZAO0AAAAMRSUAAGAsdgcAAGAodgcAAAAjUQkAABjL8EIASQAAwGCGZwG0AwAAMBSVAACAsdgdAACAodgdAAAAjEQlAABgLMMLASQBAACDGZ4F0A4AAMBQVAIAAMZidwAAAIZidwAAADASlQAAgLEMLwSQBAAADGZ4FkA7AAAAQ1EJAAAYi90BAAAYit0BAADASCQBAABjWXx4eGP69OmyWCxuR7t27Vznjx49quTkZDVs2FD16tVTYmKiCgoKzuSrnhRJAADAXP7KAiRddtllys/Pdx2ff/6569z48eO1cuVKvfvuu8rKytLBgwc1ePDgan/NU+GaAAAA/CAoKEjR0dEnjBcVFen111/X0qVLdcMNN0iSFi5cqPbt22vjxo3q1q2bz2KgEgAAMJbFh/9469tvv1VMTIxatmypIUOGaN++fZKknJwclZeXq0+fPq657dq1U7NmzZSdne2z7y5RCQAAGMyXuwMcDoccDofbmNVqldVqPWFu165dlZ6errZt2yo/P18zZsxQ9+7d9fXXX8tutys4OFjh4eFu74mKipLdbvddwKISAACAT6SlpSksLMztSEtLO+nc/v3767bbblPHjh2VkJCgVatWqbCwUO+8885ZjZlKAADAWL68TUBqaqpSUlLcxk5WBTiZ8PBwXXrppcrLy1Pfvn1VVlamwsJCt2pAQUHBSa8hOBNUAgAAxrJYfHdYrVaFhoa6HZ4mASUlJdqzZ48aN26suLg41apVS2vWrHGdz83N1b59+2Sz2Xz6/akEAABwlk2YMEE333yzmjdvroMHD2ratGkKDAzUXXfdpbCwMI0YMUIpKSmKiIhQaGioxo0bJ5vN5tOdARJJAADAaP65b/CBAwd011136eeff1ajRo103XXXaePGjWrUqJEkac6cOQoICFBiYqIcDocSEhL08ssv+zwOi9PpdPp8VT87WuHvCICad7jYcfpJwHmuaYRn5fTq+rGwzGdrNQkP9tlaZwvXBAAAYCjaAQAAYxn+EEGSAACAuXiUMAAAMBKVAACAsapzz/8LCUkAAMBcZucAtAMAADAVlQAAgLEMLwSQBAAAzMXuAAAAYCQqAQAAY7E7AAAAU5mdA9AOAADAVFQCAADGMrwQQBIAADAXuwMAAICRqAQAAIzF7gAAAAxFOwAAABiJJAAAAEPRDgAAGIt2AAAAMBKVAACAsdgdAACAoWgHAAAAI1EJAAAYy/BCAEkAAMBghmcBtAMAADAUlQAAgLHYHQAAgKHYHQAAAIxEJQAAYCzDCwEkAQAAgxmeBdAOAADAUFQCAADGYncAAACGYncAAAAwksXpdDr9HQTObw6HQ2lpaUpNTZXVavV3OECN4O85LkQkAThjxcXFCgsLU1FRkUJDQ/0dDlAj+HuOCxHtAAAADEUSAACAoUgCAAAwFEkAzpjVatW0adO4WAoXNP6e40LEhYEAABiKSgAAAIYiCQAAwFAkAQAAGIokAAAAQ5EE4IzNmzdPl1xyiWrXrq2uXbtq06ZN/g4J8Jn169fr5ptvVkxMjCwWi1asWOHvkACfIQnAGXn77beVkpKiadOmaevWrerUqZMSEhJ06NAhf4cG+ERpaak6deqkefPm+TsUwOfYIogz0rVrV1111VV66aWXJElVVVVq2rSpxo0bp0cffdTP0QG+ZbFYtHz5cg0aNMjfoQA+QSUA1VZWVqacnBz16dPHNRYQEKA+ffooOzvbj5EBADxBEoBq++mnn1RZWamoqCi38aioKNntdj9FBQDwFEkAAACGIglAtV100UUKDAxUQUGB23hBQYGio6P9FBUAwFMkAai24OBgxcXFac2aNa6xqqoqrVmzRjabzY+RAQA8EeTvAHB+S0lJUVJSkq688kpdffXVev7551VaWqrhw4f7OzTAJ0pKSpSXl+d6vXfvXm3fvl0RERFq1qyZHyMDzhxbBHHGXnrpJT3zzDOy2+3q3Lmz5s6dq65du/o7LMAn1q1bp169ep0wnpSUpPT09LMfEOBDJAEAABiKawIAADAUSQAAAIYiCQAAwFAkAQAAGIokAAAAQ5EEAABgKJIAAAAMRRIA+NCwYcPcnjV//fXX66GHHjrrcaxbt04Wi0WFhYWnnGOxWLRixQqP15w+fbo6d+58RnF9//33slgs2r59+xmtA8A3SAJwwRs2bJgsFossFouCg4PVunVrzZw5UxUVFTX+2R988IFmzZrl0VxPfnADgC/x7AAYoV+/flq4cKEcDodWrVql5ORk1apVS6mpqSfMLSsrU3BwsE8+NyIiwifrAEBNoBIAI1itVkVHR6t58+YaM2aM+vTpow8//FDS/1/Cf+KJJxQTE6O2bdtKkvbv36/bb79d4eHhioiI0MCBA/X999+71qysrFRKSorCw8PVsGFDPfLII/rfu3D/bzvA4XBo0qRJatq0qaxWq1q3bq3XX39d33//vev+9A0aNJDFYtGwYcMkHXsyY1pamlq0aKE6deqoU6dOeu+999w+Z9WqVbr00ktVp04d9erVyy1OT02aNEmXXnqp6tatq5YtW2rKlCkqLy8/Yd4rr7yipk2bqm7durr99ttVVFTkdv61115T+/btVbt2bbVr104vv/zyKT/zl19+0ZAhQ9SoUSPVqVNHbdq00cKFC72OHUD1UAmAkerUqaOff/7Z9XrNmjUKDQ1VZmamJKm8vFwJCQmy2Wz67LPPFBQUpMcff1z9+vXTV199peDgYD333HNKT0/XG2+8ofbt2+u5557T8uXLdcMNN5zyc4cOHars7GzNnTtXnTp10t69e/XTTz+padOmev/995WYmKjc3FyFhoaqTp06kqS0tDQtXrxYCxYsUJs2bbR+/Xrdc889atSokXr27Kn9+/dr8ODBSk5O1qhRo7RlyxY9/PDDXv9vUr9+faWnpysmJkY7duzQyJEjVb9+fT3yyCOuOXl5eXrnnXe0cuVKFRcXa8SIEbr//vu1ZMkSSdKSJUs0depUvfTSS+rSpYu2bdumkSNHKiQkRElJSSd85pQpU/TNN99o9erVuuiii5SXl6fff//d69gBVJMTuMAlJSU5Bw4c6HQ6nc6qqipnZmam02q1OidMmOA6HxUV5XQ4HK73/OMf/3C2bdvWWVVV5RpzOBzOOnXqOD/++GOn0+l0Nm7c2Dl79mzX+fLycufFF1/s+iyn0+ns2bOn88EHH3Q6nU5nbm6uU5IzMzPzpHF++umnTknOX375xTV29OhRZ926dZ0bNmxwmztixAjnXXfd5XQ6nc7U1FRnbGys2/lJkyadsNb/kuRcvnz5Kc8/88wzzri4ONfradOmOQMDA50HDhxwja1evdoZEBDgzM/PdzqdTmerVq2cS5cudVtn1qxZTpvN5nQ6nc69e/c6JTm3bdvmdDqdzptvvtk5fPjwU8YAoGZRCYARMjIyVK9ePZWXl6uqqkp33323pk+f7jrfoUMHt+sAvvzyS+Xl5al+/fpu6xw9elR79uxRUVGR8vPz3R6ZHBQUpCuvvPKElsBx27dvV2BgoHr27Olx3Hl5efrtt9/Ut29ft/GysjJ16dJFkrRr164THt1ss9k8/ozj3n77bc2dO1d79uxRSUmJKioqFBoa6janWbNmatKkidvnVFVVKTc3V/Xr19eePXs0YsQIjRw50jWnoqJCYWFhJ/3MMWPGKDExUVu3blV8fLwGDRqka665xuvYAVQPSQCM0KtXL82fP1/BwcGKiYlRUJD7X/2QkBC31yUlJYqLi3OVuf+oUaNG1YrheHnfGyUlJZKkf/3rX24/fKVj1zn4SnZ2toYMGaIZM2YoISFBYWFhWrZsmZ577jmvY3311VdPSEoCAwNP+p7+/fvrhx9+0KpVq5SZmanevXsrOTlZzz77bPW/DACPkQTACCEhIWrdurXH86+44gq9/fbbioyMPOG34eMaN26sL774Qj169JB07DfenJwcXXHFFSed36FDB1VVVSkrK0t9+vQ54fzxSkRlZaVrLDY2VlarVfv27TtlBaF9+/auixyP27hx4+m/5B9s2LBBzZs312OPPeYa++GHH06Yt2/fPh08eFAxMTGuzwkICFDbtm0VFRWlmJgYfffddxoyZIjHn92oUSMlJSUpKSlJ3bt318SJE0kCgLOE3QHASQwZMkQXXXSRBg4cqM8++0x79+7VunXr9MADD+jAgQOSpAcffFBPPfWUVqxYod27d+v+++//0z3+l1xyiZKSkvTXv/5VK1ascK35zjvvSJKaN28ui8WijIwMHT58WCUlJapfv74mTJig8ePHa9GiRdqzZ4+2bt2qF198UYsWLZIkjR49Wt9++60mTpyo3NxcLV26VOnp6V593zZt2mjfvn1atmyZ9uzZo7lz52r58uUnzKtdu7aSkpL05Zdf6rPPPtMDDzyg22+/XdHR0ZKkGTNmKC0tTXPnztV///tf7dixQwsXLtTf/va3k37u1KlT9c9//lN5eXnauXOnMjIy1L59e69iB1B9JAHASdStW1fr169Xs2bNNHjwYLVv314jRozQ0aNHXZWBhx9+WPfee6+SkpJks9lUv359/eUvf/nTdefPn69bb71V999/v9q1a6eRI0eqtLRUktSkSRPNmDFDjz76qKKiojR27FhJ0qxZszRlyhSlpaWpffv26tevn/71r3+pRYsWko716d9//32tWLFCnTp10oIFC/Tkk0969X1vueUWjR8/XmPHjlXnzp21YcMGTZky5YR5rVu31uDBg3XjjTcqPj5eHTt2dNsCeN999+m1117TwoUL1aFDB/Xs2VPp6emuWP9XcHCwUlNT1bFjR/Xo0UOBgYFatmyZV7EDqD6L81RXMQEAgAsalQAAAAxFEgAAgKFIAgAAMBRJAAAAhiIJAADAUCQBAAAYiiQAAABDkQQAAGAokgAAAAxFEgAAgKFIAgAAMBRJAAAAhvr/AIFciuvy3Z/IAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_siamese_network(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_test = eval_data.sample(1)\n",
    "# img1, img2, label = random_test.iloc[0]\n",
    "# img1 = img1.strip(\"()\")\n",
    "# img2 = img2.strip(\"()\")\n",
    "\n",
    "# # Load images\n",
    "# image1 = Image.open(img1).convert(\"RGB\")\n",
    "# image2 = Image.open(img2).convert(\"RGB\")\n",
    "\n",
    "# # Print Images\n",
    "# display(image1)\n",
    "# display(image2)\n",
    "\n",
    "# # Apply transformations\n",
    "# image1 = transform(image1).unsqueeze(0)\n",
    "# image2 = transform(image2).unsqueeze(0)\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = trained_model(image1, image2)\n",
    "\n",
    "# # Calculate the euclidean distance between the outputs\n",
    "# # diff = output1 - output2\n",
    "# # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "# # dist = torch.sqrt(dist_sq)\n",
    "# dist = F.pairwise_distance(output1, output2)\n",
    "# print(f\"Distance: {dist.item()}\")\n",
    "\n",
    "# # Get predictions\n",
    "# predicted = (dist < 1.0).float()\n",
    "# true_label = 0 if label < 2 else 1\n",
    "\n",
    "# print(f\"Predicted: {predicted.item()}, Actual: {true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca of the output:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the output of the model\n",
    "outputs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img1, img2, label in eval_loader:\n",
    "        output1, output2 = trained_model(img1, img2)\n",
    "        outputs.append(output1)\n",
    "        labels.append(label)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_outputs = pca.fit_transform(outputs)\n",
    "\n",
    "# Plot the PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_outputs[:, 0], y=pca_outputs[:, 1], hue=labels, palette='viridis')\n",
    "plt.title('PCA of the Siamese Network Output')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled rows:  (33288, 5)\n"
     ]
    }
   ],
   "source": [
    "path_data_pool = \"datasets/house_styles/sampled_paired_labels_shuffled.csv\"\n",
    "data_pool = pd.read_csv(path_data_pool)\n",
    "unlabeled_rows = data_pool[data_pool['similarity'].isna()]\n",
    "print(\"Unlabeled rows: \", unlabeled_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [13:10<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('datasets/house_styles/all_images/108_c76f77e2.jpg', 'datasets/house_styles/all_images/034_ad26fac6.jpg', 0.9999391436576843, 6.085634231567383e-05), ('datasets/house_styles/all_images/427_31f4d6f2.jpg', 'datasets/house_styles/all_images/366_6a85a605.jpg', 0.999911904335022, 8.809566497802734e-05), ('datasets/house_styles/all_images/310_10bdb154.jpg', 'datasets/house_styles/all_images/382_761139e9.jpg', 0.9997299313545227, 0.0002700686454772949), ('datasets/house_styles/all_images/388_aa0bd42a.jpg', 'datasets/house_styles/all_images/360_28ef686b.jpg', 0.9995713233947754, 0.0004286766052246094), ('datasets/house_styles/all_images/142_a3bbdd3d.jpg', 'datasets/house_styles/all_images/123_aa386d79.jpg', 0.9995629191398621, 0.00043708086013793945), ('datasets/house_styles/all_images/077_fd3da9e2.jpg', 'datasets/house_styles/all_images/035_6e7ff66b.jpg', 1.0005346536636353, 0.0005346536636352539), ('datasets/house_styles/all_images/212_aea88577.jpg', 'datasets/house_styles/all_images/087_24c73d10.jpg', 1.0006413459777832, 0.0006413459777832031), ('datasets/house_styles/all_images/212_aea88577.jpg', 'datasets/house_styles/all_images/125_743047fb.jpg', 1.0006413459777832, 0.0006413459777832031), ('datasets/house_styles/all_images/155_732ad883.jpg', 'datasets/house_styles/all_images/153_acb5cd48.jpg', 0.9992117285728455, 0.000788271427154541), ('datasets/house_styles/all_images/105_1c926771.jpg', 'datasets/house_styles/all_images/315_59373113.jpg', 0.9990601539611816, 0.0009398460388183594), ('datasets/house_styles/all_images/081_156ea596.jpg', 'datasets/house_styles/all_images/057_28786773.jpg', 0.9987893104553223, 0.0012106895446777344), ('datasets/house_styles/all_images/202_55578dea.jpg', 'datasets/house_styles/all_images/082_40db5260.jpg', 1.0013736486434937, 0.0013736486434936523), ('datasets/house_styles/all_images/304_7774d253.jpg', 'datasets/house_styles/all_images/112_f21e8af7.jpg', 0.9985216856002808, 0.0014783143997192383), ('datasets/house_styles/all_images/384_7bba4266.jpg', 'datasets/house_styles/all_images/291_87a33ca7.jpg', 1.0014984607696533, 0.0014984607696533203), ('datasets/house_styles/all_images/097_41e7ad48.jpg', 'datasets/house_styles/all_images/221_cbdbfa68.jpg', 1.0015168190002441, 0.0015168190002441406), ('datasets/house_styles/all_images/221_cbdbfa68.jpg', 'datasets/house_styles/all_images/138_03bf52ae.jpg', 1.001518726348877, 0.0015187263488769531), ('datasets/house_styles/all_images/097_41e7ad48.jpg', 'datasets/house_styles/all_images/136_9f1a39c1.jpg', 0.9984342455863953, 0.0015657544136047363), ('datasets/house_styles/all_images/037_326d0815.jpg', 'datasets/house_styles/all_images/060_26c18ddf.jpg', 0.9983698725700378, 0.0016301274299621582), ('datasets/house_styles/all_images/168_bbf12d12.jpg', 'datasets/house_styles/all_images/171_18d3b628.jpg', 1.0017313957214355, 0.0017313957214355469), ('datasets/house_styles/all_images/201_edbb9f6e.jpg', 'datasets/house_styles/all_images/225_25606f44.jpg', 1.0018975734710693, 0.001897573471069336), ('datasets/house_styles/all_images/225_25606f44.jpg', 'datasets/house_styles/all_images/199_e4e5df84.jpg', 1.0018997192382812, 0.00189971923828125), ('datasets/house_styles/all_images/018_7868b4d7.jpg', 'datasets/house_styles/all_images/510_95629ee6.jpg', 1.0022354125976562, 0.00223541259765625), ('datasets/house_styles/all_images/294_3260f65d.jpg', 'datasets/house_styles/all_images/153_acb5cd48.jpg', 0.9977177977561951, 0.0022822022438049316), ('datasets/house_styles/all_images/201_edbb9f6e.jpg', 'datasets/house_styles/all_images/019_61d9365b.jpg', 1.0023597478866577, 0.002359747886657715), ('datasets/house_styles/all_images/257_f1b4cd5b.jpg', 'datasets/house_styles/all_images/075_7b7e25b1.jpg', 0.9973691701889038, 0.0026308298110961914), ('datasets/house_styles/all_images/125_b3f68f40.jpg', 'datasets/house_styles/all_images/010_0e38a333.jpg', 0.9971349835395813, 0.002865016460418701), ('datasets/house_styles/all_images/159_f211f05e.jpg', 'datasets/house_styles/all_images/201_edbb9f6e.jpg', 0.9970706105232239, 0.002929389476776123), ('datasets/house_styles/all_images/159_f211f05e.jpg', 'datasets/house_styles/all_images/199_e4e5df84.jpg', 0.9970706105232239, 0.002929389476776123), ('datasets/house_styles/all_images/081_156ea596.jpg', 'datasets/house_styles/all_images/230_4add03cc.jpg', 1.0029361248016357, 0.002936124801635742), ('datasets/house_styles/all_images/005_eda52226.jpg', 'datasets/house_styles/all_images/122_e44a0cb3.jpg', 1.0029789209365845, 0.0029789209365844727), ('datasets/house_styles/all_images/168_bbf12d12.jpg', 'datasets/house_styles/all_images/201_936151d8.jpg', 0.9969132542610168, 0.0030867457389831543), ('datasets/house_styles/all_images/469_1ad24162.jpg', 'datasets/house_styles/all_images/501_bcd7edcd.jpg', 1.003410816192627, 0.003410816192626953), ('datasets/house_styles/all_images/050_999633cb.jpg', 'datasets/house_styles/all_images/388_aa0bd42a.jpg', 0.9965720176696777, 0.0034279823303222656), ('datasets/house_styles/all_images/041_d7474487.jpg', 'datasets/house_styles/all_images/035_7b4de4fd.jpg', 0.9964442849159241, 0.0035557150840759277), ('datasets/house_styles/all_images/172_fb1c801f.jpg', 'datasets/house_styles/all_images/077_7688f758.jpg', 1.0035932064056396, 0.0035932064056396484), ('datasets/house_styles/all_images/128_841a4e49.jpg', 'datasets/house_styles/all_images/150_1454420c.jpg', 1.0037213563919067, 0.0037213563919067383), ('datasets/house_styles/all_images/257_f1b4cd5b.jpg', 'datasets/house_styles/all_images/206_9b5a908f.jpg', 1.0037527084350586, 0.0037527084350585938), ('datasets/house_styles/all_images/117_d5ce16c2.jpg', 'datasets/house_styles/all_images/108_c76f77e2.jpg', 1.0038291215896606, 0.0038291215896606445), ('datasets/house_styles/all_images/366_08eff319.jpg', 'datasets/house_styles/all_images/229_f79304e9.jpg', 1.0038596391677856, 0.0038596391677856445), ('datasets/house_styles/all_images/113_3b341645.jpg', 'datasets/house_styles/all_images/069_3a6f2b43.jpg', 0.9960522651672363, 0.003947734832763672), ('datasets/house_styles/all_images/135_8c978f4e.jpg', 'datasets/house_styles/all_images/096_0dc661af.jpg', 1.0040465593338013, 0.0040465593338012695), ('datasets/house_styles/all_images/095_0a434ed3.jpg', 'datasets/house_styles/all_images/020_c8201633.jpg', 0.9958760142326355, 0.004123985767364502), ('datasets/house_styles/all_images/163_83960f4c.jpg', 'datasets/house_styles/all_images/010_0e38a333.jpg', 0.9957304000854492, 0.004269599914550781), ('datasets/house_styles/all_images/180_ea16e99b.jpg', 'datasets/house_styles/all_images/058_7f8c2954.jpg', 0.995703935623169, 0.004296064376831055), ('datasets/house_styles/all_images/168_bbf12d12.jpg', 'datasets/house_styles/all_images/139_38ae72c7.jpg', 1.004300832748413, 0.004300832748413086), ('datasets/house_styles/all_images/094_c29b2dc0.jpg', 'datasets/house_styles/all_images/304_56934f95.jpg', 0.9956626892089844, 0.004337310791015625), ('datasets/house_styles/all_images/284_1d326f0c.jpg', 'datasets/house_styles/all_images/257_f1b4cd5b.jpg', 1.0043531656265259, 0.004353165626525879), ('datasets/house_styles/all_images/227_33071f4b.jpg', 'datasets/house_styles/all_images/301_b73b9663.jpg', 1.0043554306030273, 0.004355430603027344), ('datasets/house_styles/all_images/196_95e8f791.jpg', 'datasets/house_styles/all_images/485_1c18da79.jpg', 0.9953823089599609, 0.0046176910400390625), ('datasets/house_styles/all_images/128_841a4e49.jpg', 'datasets/house_styles/all_images/198_7df0d8be.jpg', 0.9953314661979675, 0.004668533802032471), ('datasets/house_styles/all_images/187_f9d3cf18.jpg', 'datasets/house_styles/all_images/284_404c75b4.jpg', 0.9952367544174194, 0.004763245582580566), ('datasets/house_styles/all_images/349_60f37449.jpg', 'datasets/house_styles/all_images/343_354bff03.jpg', 1.005010962486267, 0.00501096248626709), ('datasets/house_styles/all_images/081_21153085.jpg', 'datasets/house_styles/all_images/382_baa9260f.jpg', 1.005065679550171, 0.0050656795501708984), ('datasets/house_styles/all_images/011_cc99985c.jpg', 'datasets/house_styles/all_images/331_377860bc.jpg', 0.994778037071228, 0.005221962928771973), ('datasets/house_styles/all_images/080_1bdfee05.jpg', 'datasets/house_styles/all_images/106_253755c1.jpg', 1.0054200887680054, 0.005420088768005371), ('datasets/house_styles/all_images/080_1bdfee05.jpg', 'datasets/house_styles/all_images/096_0dc661af.jpg', 1.0054200887680054, 0.005420088768005371), ('datasets/house_styles/all_images/054_37cbf3d3.jpg', 'datasets/house_styles/all_images/469_1ad24162.jpg', 1.00552237033844, 0.005522370338439941), ('datasets/house_styles/all_images/068_dc83e73a.jpg', 'datasets/house_styles/all_images/346_e98ba7a9.jpg', 0.9944660067558289, 0.005533993244171143), ('datasets/house_styles/all_images/117_d5ce16c2.jpg', 'datasets/house_styles/all_images/147_579b3c29.jpg', 1.0055651664733887, 0.005565166473388672), ('datasets/house_styles/all_images/384_7bba4266.jpg', 'datasets/house_styles/all_images/190_48347536.jpg', 0.9943639039993286, 0.005636096000671387), ('datasets/house_styles/all_images/116_32f01ef6.jpg', 'datasets/house_styles/all_images/084_dae155bc.jpg', 1.0057165622711182, 0.005716562271118164), ('datasets/house_styles/all_images/128_841a4e49.jpg', 'datasets/house_styles/all_images/159_7b4fd689.jpg', 1.0060503482818604, 0.0060503482818603516), ('datasets/house_styles/all_images/098_cff86baa.jpg', 'datasets/house_styles/all_images/035_6e7ff66b.jpg', 1.0062470436096191, 0.006247043609619141), ('datasets/house_styles/all_images/363_12412886.jpg', 'datasets/house_styles/all_images/208_78b28f8b.jpg', 0.9933710098266602, 0.006628990173339844), ('datasets/house_styles/all_images/439_ac9a7940.jpg', 'datasets/house_styles/all_images/301_b73b9663.jpg', 0.993216872215271, 0.006783127784729004), ('datasets/house_styles/all_images/017_ffb23173.jpg', 'datasets/house_styles/all_images/507_7d484885.jpg', 1.0076111555099487, 0.0076111555099487305), ('datasets/house_styles/all_images/105_1c926771.jpg', 'datasets/house_styles/all_images/501_bcd7edcd.jpg', 1.0077753067016602, 0.007775306701660156), ('datasets/house_styles/all_images/397_54b2527d.jpg', 'datasets/house_styles/all_images/360_28ef686b.jpg', 0.9921661019325256, 0.007833898067474365), ('datasets/house_styles/all_images/095_3cbc2895.jpg', 'datasets/house_styles/all_images/109_4eafbe2a.jpg', 1.0079643726348877, 0.007964372634887695), ('datasets/house_styles/all_images/115_32cb3529.jpg', 'datasets/house_styles/all_images/216_8570669c.jpg', 1.0079905986785889, 0.007990598678588867), ('datasets/house_styles/all_images/025_bea784dd.jpg', 'datasets/house_styles/all_images/105_1c926771.jpg', 0.9919434785842896, 0.00805652141571045), ('datasets/house_styles/all_images/122_e44a0cb3.jpg', 'datasets/house_styles/all_images/236_1c4cda1e.jpg', 0.9919298887252808, 0.008070111274719238), ('datasets/house_styles/all_images/112_f21e8af7.jpg', 'datasets/house_styles/all_images/250_27aa4502.jpg', 0.9918265342712402, 0.008173465728759766), ('datasets/house_styles/all_images/194_a66427af.jpg', 'datasets/house_styles/all_images/151_d0e35309.jpg', 0.9916986227035522, 0.008301377296447754), ('datasets/house_styles/all_images/012_f54e5947.jpg', 'datasets/house_styles/all_images/040_6ddfd3e7.jpg', 1.0085303783416748, 0.008530378341674805), ('datasets/house_styles/all_images/143_606cf1bf.jpg', 'datasets/house_styles/all_images/037_326d0815.jpg', 0.9914556741714478, 0.008544325828552246), ('datasets/house_styles/all_images/105_1c926771.jpg', 'datasets/house_styles/all_images/019_61d9365b.jpg', 0.9910274147987366, 0.008972585201263428), ('datasets/house_styles/all_images/202_698495e0.jpg', 'datasets/house_styles/all_images/334_493d0c20.jpg', 0.9908892512321472, 0.009110748767852783), ('datasets/house_styles/all_images/366_6a85a605.jpg', 'datasets/house_styles/all_images/153_acb5cd48.jpg', 0.9908310174942017, 0.00916898250579834), ('datasets/house_styles/all_images/005_eda52226.jpg', 'datasets/house_styles/all_images/083_1941f4a1.jpg', 0.9908202290534973, 0.009179770946502686), ('datasets/house_styles/all_images/447_96c74801.jpg', 'datasets/house_styles/all_images/196_4a29195b.jpg', 1.0092356204986572, 0.009235620498657227), ('datasets/house_styles/all_images/053_9f014739.jpg', 'datasets/house_styles/all_images/156_55935f6b.jpg', 1.009600281715393, 0.009600281715393066), ('datasets/house_styles/all_images/018_d675e0cc.jpg', 'datasets/house_styles/all_images/035_6e7ff66b.jpg', 0.9902949333190918, 0.009705066680908203), ('datasets/house_styles/all_images/034_ad26fac6.jpg', 'datasets/house_styles/all_images/037_326d0815.jpg', 1.009778618812561, 0.009778618812561035), ('datasets/house_styles/all_images/083_b149d914.jpg', 'datasets/house_styles/all_images/304_56934f95.jpg', 1.0099284648895264, 0.009928464889526367), ('datasets/house_styles/all_images/181_7a55eb4a.jpg', 'datasets/house_styles/all_images/050_999633cb.jpg', 0.9900439977645874, 0.009956002235412598), ('datasets/house_styles/all_images/155_732ad883.jpg', 'datasets/house_styles/all_images/429_bd3086cc.jpg', 1.0101821422576904, 0.01018214225769043), ('datasets/house_styles/all_images/128_841a4e49.jpg', 'datasets/house_styles/all_images/188_9b067c58.jpg', 1.0103169679641724, 0.010316967964172363), ('datasets/house_styles/all_images/163_de1fdac4.jpg', 'datasets/house_styles/all_images/212_aea88577.jpg', 1.0103567838668823, 0.010356783866882324), ('datasets/house_styles/all_images/247_7ebd437b.jpg', 'datasets/house_styles/all_images/123_d5fed066.jpg', 1.0103938579559326, 0.010393857955932617), ('datasets/house_styles/all_images/142_12fc668a.jpg', 'datasets/house_styles/all_images/470_6e733604.jpg', 1.0104728937149048, 0.010472893714904785), ('datasets/house_styles/all_images/159_f211f05e.jpg', 'datasets/house_styles/all_images/410_43ec7ac3.jpg', 1.010595679283142, 0.01059567928314209), ('datasets/house_styles/all_images/011_cc99985c.jpg', 'datasets/house_styles/all_images/363_12412886.jpg', 0.9892858862876892, 0.010714113712310791), ('datasets/house_styles/all_images/105_1c926771.jpg', 'datasets/house_styles/all_images/069_3a6f2b43.jpg', 0.9891566038131714, 0.010843396186828613), ('datasets/house_styles/all_images/384_7bba4266.jpg', 'datasets/house_styles/all_images/284_1d326f0c.jpg', 1.0110979080200195, 0.011097908020019531), ('datasets/house_styles/all_images/399_33c5d6dd.jpg', 'datasets/house_styles/all_images/019_61d9365b.jpg', 0.9888313412666321, 0.01116865873336792), ('datasets/house_styles/all_images/095_3cbc2895.jpg', 'datasets/house_styles/all_images/159_f211f05e.jpg', 1.011469841003418, 0.011469841003417969), ('datasets/house_styles/all_images/159_f211f05e.jpg', 'datasets/house_styles/all_images/129_eef5c8a1.jpg', 1.011471152305603, 0.011471152305603027), ('datasets/house_styles/all_images/007_64a3e2e9.jpg', 'datasets/house_styles/all_images/507_7d484885.jpg', 1.0116046667099, 0.011604666709899902), ('datasets/house_styles/all_images/376_3473d618.jpg', 'datasets/house_styles/all_images/257_f1b4cd5b.jpg', 0.9883546233177185, 0.011645376682281494)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_rows = unlabeled_rows[['image1_path', 'image2_path', 'similarity']].sample(5000)\n",
    "unlabeled_dataset = ImageSimilarityDataset(unlabeled_rows, transform=clip_transform)\n",
    "data_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def active_learning_confident_samples(model, dataloader, margin=1.0, budget=120):\n",
    "    \"\"\"\n",
    "    Identify the least confident samples from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Siamese network.\n",
    "        dataloader: DataLoader for the dataset you want to evaluate.\n",
    "        margin: The margin used in the contrastive loss.\n",
    "        top_k: Number of least confident samples to return.\n",
    "    \n",
    "    Returns:\n",
    "        A list of the top_k least confident samples (input pairs and distances).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    least_confident_samples = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for img1_path, img2_path, img1, img2, labels in tqdm(dataloader):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, label = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Get the model outputs for both images\n",
    "            output1, output2 = model(img1, img2)\n",
    "            \n",
    "            # Calculate pairwise distance\n",
    "            distances = F.pairwise_distance(output1, output2)\n",
    "            \n",
    "            # Calculate confidence score (distance from the margin)\n",
    "            confidence_scores = torch.abs(distances - margin)\n",
    "\n",
    "            # Collect the least confident samples (small confidence score means high uncertainty)\n",
    "            for i in range(len(confidence_scores)):\n",
    "                least_confident_samples.append((img1_path[i], img2_path[i], distances[i].item(), confidence_scores[i].item()))\n",
    "\n",
    "    # Sort samples by confidence score (ascending, to get least confident samples)\n",
    "    least_confident_samples.sort(key=lambda x: x[3])\n",
    "\n",
    "    # Return the top_k least confident samples\n",
    "    return least_confident_samples[:budget]\n",
    "\n",
    "least_confident_samples = active_learning_confident_samples(trained_model, data_loader, margin=1.0, budget=100)\n",
    "print(least_confident_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save least confident samples to be tagged into a csv\n",
    "least_confident_samples_df = pd.DataFrame(least_confident_samples, columns=['image1_path', 'image2_path', 'distance', 'confidence'])\n",
    "# add image names:\n",
    "least_confident_samples_df['image1'] = least_confident_samples_df['image1_path'].apply(lambda x: x.split('/')[-1])\n",
    "least_confident_samples_df['image2'] = least_confident_samples_df['image2_path'].apply(lambda x: x.split('/')[-1])\n",
    "# make paths be surrounded by parentheses:\n",
    "least_confident_samples_df['image1_path'] = least_confident_samples_df['image1_path'].apply(lambda x: '(' + x + ')')\n",
    "least_confident_samples_df['image2_path'] = least_confident_samples_df['image2_path'].apply(lambda x: '(' + x + ')')\n",
    "# shorten distance and confidence:\n",
    "least_confident_samples_df['distance'] = least_confident_samples_df['distance'].apply(lambda x: round(x, 4))\n",
    "least_confident_samples_df['confidence'] = least_confident_samples_df['confidence'].apply(lambda x: round(x, 4))\n",
    "# add similarity column:\n",
    "least_confident_samples_df['similarity'] = None\n",
    "\n",
    "# reorder columns:\n",
    "least_confident_samples_df = least_confident_samples_df[['image1', 'image1_path', 'image2', 'image2_path', 'distance', 'confidence', 'similarity']]\n",
    "\n",
    "least_confident_samples_df.to_csv('active_learning_labels/round_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
