{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Similiarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of choosing the similarity function that determines the images that are similar to a given image.\n",
    "The implementation of the network is based on the Siamese network implementation. This class of networks is known to be more robust to class imbalance, so it fits the data on which we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         image1_path  \\\n",
      "0  (datasets/house_styles/all_images/001_d2c7428a...   \n",
      "1  (datasets/house_styles/all_images/453_d7b5d246...   \n",
      "2  (datasets/house_styles/all_images/116_32f01ef6...   \n",
      "3  (datasets/house_styles/all_images/301_b73b9663...   \n",
      "4  (datasets/house_styles/all_images/042_06b56791...   \n",
      "\n",
      "                                         image2_path  similarity  \n",
      "0  (datasets/house_styles/all_images/366_08eff319...         3.0  \n",
      "1  (datasets/house_styles/all_images/122_e44a0cb3...         0.0  \n",
      "2  (datasets/house_styles/all_images/174_55a7b3f9...         0.0  \n",
      "3  (datasets/house_styles/all_images/116_32f01ef6...         0.0  \n",
      "4  (datasets/house_styles/all_images/069_d3bedc1f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "base_path = 'active_learning_labels/'\n",
    "full_data_paths = pd.read_csv(base_path + 'round_0.csv')\n",
    "\n",
    "current_round = 1\n",
    "\n",
    "if current_round > 0:\n",
    "    for i in range(1, current_round):\n",
    "        path = base_path + 'round_' + str(i) + '.csv'\n",
    "        data = pd.read_csv(path)\n",
    "        full_data_paths = pd.concat([full_data_paths, data], ignore_index=True)\n",
    "\n",
    "data_paths = full_data_paths[['image1_path', 'image2_path', 'similarity']]\n",
    "\n",
    "print(data_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (566, 3) Eval:  (142, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "train_data = data_paths.sample(frac=0.8, random_state=42)\n",
    "eval_data = data_paths.drop(train_data.index)\n",
    "\n",
    "print(\"Train: \", train_data.shape, \"Eval: \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Computer Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CLIP model and processor from Hugging Face\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set up the image transformation pipeline\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Define the dataset class\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomApply([transforms.RandomResizedCrop(224)], p=0.2),  # 20% chance of random resized crop\n",
    "    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.2),  # 20% chance of horizontal flip\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.2),  # 20% chance of color jitter\n",
    "    transforms.ToTensor(),  # Always apply ToTensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=clip_transform, augmentations=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        self.master_path = ''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from the paths\n",
    "        image1_path = self.master_path + self.data.iloc[idx, 0].strip(\"()\")\n",
    "        image2_path = self.master_path + self.data.iloc[idx, 1].strip(\"()\")\n",
    "        \n",
    "        # Load images\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentations:\n",
    "            image1 = self.augmentations(image1)\n",
    "            image2 = self.augmentations(image2)\n",
    "\n",
    "        # CLIP:\n",
    "        images_features = []\n",
    "        for img in [image1, image2]:\n",
    "            image_tensor = self.transform(img).unsqueeze(0)\n",
    "            inputs = processor(images=image_tensor, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "                #images_features.append(image_features.numpy().flatten())\n",
    "            images_features.append(image_features.squeeze())  # Ensure it's a 512-dimensional tensor\n",
    "\n",
    "        \n",
    "        # # Apply transforms if provided\n",
    "        # if self.transform:\n",
    "        #     image1 = self.transform(image1)\n",
    "        #     image2 = self.transform(image2)\n",
    "        \n",
    "        # Get similarity score\n",
    "        similarity = self.data.iloc[idx, 2]\n",
    "        label = 0 if similarity < 3 else 1\n",
    "        \n",
    "        return image1_path, image2_path, images_features[0], images_features[1], torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageSimilarityDataset(train_data, transform=clip_transform, augmentations=None)\n",
    "eval_dataset = ImageSimilarityDataset(eval_data, transform=clip_transform, augmentations=None)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)  # Keep larger dimension here\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Using fully connected layers to process 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # First layer to reduce dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),  # Second layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)     # Output layer with 2 units (for similarity comparison)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         return self.fc1(x)\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass for both inputs\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Use fully connected layers for processing 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # Assuming CLIP outputs 512-dimensional features\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Setting up the Sequential of CNN Layers\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "            \n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to ensure the output size is consistent\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "#         # Defining the fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(1024, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n",
    "\n",
    "# # class SiameseNetwork(nn.Module):\n",
    "    \n",
    "# #     def __init__(self):\n",
    "# #         super(SiameseNetwork, self).__init__()\n",
    "# #         # Setting up the Sequential of CNN Layers\n",
    "# #         self.cnn1 = nn.Sequential(\n",
    "# #             nn.Conv2d(3, 96, kernel_size=11, stride=1),  # Adjusted for RGB input\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "# #             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "\n",
    "# #             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "# #         )\n",
    "        \n",
    "# #         # Adaptive pooling layer to ensure the output size is consistent\n",
    "# #         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "# #         # Defining the fully connected layers\n",
    "# #         self.fc1 = nn.Sequential(\n",
    "# #             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.Dropout(p=0.5),\n",
    "            \n",
    "# #             nn.Linear(1024, 128),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Linear(128, 2)\n",
    "# #         )\n",
    "        \n",
    "# #     def forward_once(self, x):\n",
    "# #         # Forward pass \n",
    "# #         x = self.cnn1(x)\n",
    "# #         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "# #         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#     #     x = self.fc1(x)\n",
    "#     #     return x\n",
    "\n",
    "#     # def forward(self, input1, input2):\n",
    "#     #     # Forward pass of input 1\n",
    "#     #     output1 = self.forward_once(input1)\n",
    "#     #     # Forward pass of input 2\n",
    "#     #     output2 = self.forward_once(input2)\n",
    "#     #     return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallSiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SmallSiameseNetwork, self).__init__()\n",
    "#         # Setting up a smaller CNN\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=1),  # Fewer filters, smaller kernel size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Max pooling with smaller stride to reduce spatial dimensions\n",
    "#             nn.Dropout2d(p=0.2),  # Reduced dropout rate\n",
    "            \n",
    "#             nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),  # Fewer filters\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Smaller max-pooling\n",
    "#             nn.Dropout2d(p=0.2),\n",
    "\n",
    "#             # nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.MaxPool2d(2, stride=2),\n",
    "#             # nn.Dropout2d(p=0.2),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to standardize output size (reduce to 3x3)\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))  # Smaller output size (3x3)\n",
    "        \n",
    "#         # Defining smaller fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(64 * 3 * 3, 128),  # Reduced size based on new feature map size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.4),  # Keep some dropout for regularization\n",
    "            \n",
    "#             nn.Linear(128, 64),  # Smaller fully connected layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(64, 2)  # Output layer remains the same\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        ### Binary:\n",
    "        # euclidian distance\n",
    "        # diff = x0 - x1\n",
    "        # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        # if torch.any(dist_sq < 0):\n",
    "        #     print('the value of dist_sq is negative: ' + dist_sq)\n",
    "        # # dist = torch.sqrt(torch.abs(dist_sq))\n",
    "        # dist = torch.sqrt(dist_sq + 1e-6)\n",
    "        # mdist = self.margin - dist\n",
    "        # dist = torch.clamp(mdist, min=0.0)\n",
    "        # loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        # loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "\n",
    "        label = y #binary?\n",
    "        euclidean_distance = nn.functional.pairwise_distance(x0, x1)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) + # similar\n",
    "                                (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)) # dissimilar\n",
    "        return loss_contrastive\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_network(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in tqdm(enumerate(train_loader)):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print statistics\n",
    "            # if (i + 1) % 10 == 0:  # Print every 5 batches\n",
    "            #     print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\")\n",
    "            #     running_loss = 0.0\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.01, weight_decay=0.001)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/71], Loss: 0.8791\n",
      "Epoch [1/50], Step [20/71], Loss: 0.3638\n",
      "Epoch [1/50], Step [30/71], Loss: 0.3490\n",
      "Epoch [1/50], Step [40/71], Loss: 0.2393\n",
      "Epoch [1/50], Step [50/71], Loss: 0.2248\n",
      "Epoch [1/50], Step [60/71], Loss: 0.2547\n",
      "Epoch [1/50], Step [70/71], Loss: 0.3326\n",
      "Epoch [2/50], Step [10/71], Loss: 0.1273\n",
      "Epoch [2/50], Step [20/71], Loss: 0.2405\n",
      "Epoch [2/50], Step [30/71], Loss: 0.3745\n",
      "Epoch [2/50], Step [40/71], Loss: 0.2970\n",
      "Epoch [2/50], Step [50/71], Loss: 0.2305\n",
      "Epoch [2/50], Step [60/71], Loss: 0.1739\n",
      "Epoch [2/50], Step [70/71], Loss: 0.1835\n",
      "Epoch [3/50], Step [10/71], Loss: 0.1263\n",
      "Epoch [3/50], Step [20/71], Loss: 0.1533\n",
      "Epoch [3/50], Step [30/71], Loss: 0.3624\n",
      "Epoch [3/50], Step [40/71], Loss: 0.3022\n",
      "Epoch [3/50], Step [50/71], Loss: 0.4382\n",
      "Epoch [3/50], Step [60/71], Loss: 0.2117\n",
      "Epoch [3/50], Step [70/71], Loss: 0.2662\n",
      "Epoch [4/50], Step [10/71], Loss: 0.1945\n",
      "Epoch [4/50], Step [20/71], Loss: 0.2314\n",
      "Epoch [4/50], Step [30/71], Loss: 0.3990\n",
      "Epoch [4/50], Step [40/71], Loss: 0.2323\n",
      "Epoch [4/50], Step [50/71], Loss: 0.0602\n",
      "Epoch [4/50], Step [60/71], Loss: 0.1805\n",
      "Epoch [4/50], Step [70/71], Loss: 0.1383\n",
      "Epoch [5/50], Step [10/71], Loss: 0.1928\n",
      "Epoch [5/50], Step [20/71], Loss: 0.1792\n",
      "Epoch [5/50], Step [30/71], Loss: 0.1094\n",
      "Epoch [5/50], Step [40/71], Loss: 0.1773\n",
      "Epoch [5/50], Step [50/71], Loss: 0.3234\n",
      "Epoch [5/50], Step [60/71], Loss: 0.3206\n",
      "Epoch [5/50], Step [70/71], Loss: 0.2196\n",
      "Epoch [6/50], Step [10/71], Loss: 0.5098\n",
      "Epoch [6/50], Step [20/71], Loss: 0.3355\n",
      "Epoch [6/50], Step [30/71], Loss: 0.2589\n",
      "Epoch [6/50], Step [40/71], Loss: 0.5577\n",
      "Epoch [6/50], Step [50/71], Loss: 0.1313\n",
      "Epoch [6/50], Step [60/71], Loss: 0.2744\n",
      "Epoch [6/50], Step [70/71], Loss: 0.3336\n",
      "Epoch [7/50], Step [10/71], Loss: 0.3983\n",
      "Epoch [7/50], Step [20/71], Loss: 0.5893\n",
      "Epoch [7/50], Step [30/71], Loss: 0.0903\n",
      "Epoch [7/50], Step [40/71], Loss: 0.4324\n",
      "Epoch [7/50], Step [50/71], Loss: 1.0271\n",
      "Epoch [7/50], Step [60/71], Loss: 0.2138\n",
      "Epoch [7/50], Step [70/71], Loss: 0.6484\n",
      "Epoch [8/50], Step [10/71], Loss: 0.6477\n",
      "Epoch [8/50], Step [20/71], Loss: 1.0702\n",
      "Epoch [8/50], Step [30/71], Loss: 0.7083\n",
      "Epoch [8/50], Step [40/71], Loss: 0.9666\n",
      "Epoch [8/50], Step [50/71], Loss: 0.5711\n",
      "Epoch [8/50], Step [60/71], Loss: 0.1870\n",
      "Epoch [8/50], Step [70/71], Loss: 0.2995\n",
      "Epoch [9/50], Step [10/71], Loss: 0.1236\n",
      "Epoch [9/50], Step [20/71], Loss: 0.2600\n",
      "Epoch [9/50], Step [30/71], Loss: 0.1512\n",
      "Epoch [9/50], Step [40/71], Loss: 0.1713\n",
      "Epoch [9/50], Step [50/71], Loss: 0.2395\n",
      "Epoch [9/50], Step [60/71], Loss: 0.1772\n",
      "Epoch [9/50], Step [70/71], Loss: 0.2041\n",
      "Epoch [10/50], Step [10/71], Loss: 0.1666\n",
      "Epoch [10/50], Step [20/71], Loss: 0.1572\n",
      "Epoch [10/50], Step [30/71], Loss: 0.1454\n",
      "Epoch [10/50], Step [40/71], Loss: 0.1734\n",
      "Epoch [10/50], Step [50/71], Loss: 0.1596\n",
      "Epoch [10/50], Step [60/71], Loss: 0.1960\n",
      "Epoch [10/50], Step [70/71], Loss: 0.1750\n",
      "Epoch [11/50], Step [10/71], Loss: 0.1943\n",
      "Epoch [11/50], Step [20/71], Loss: 0.0991\n",
      "Epoch [11/50], Step [30/71], Loss: 0.3159\n",
      "Epoch [11/50], Step [40/71], Loss: 0.1724\n",
      "Epoch [11/50], Step [50/71], Loss: 0.1658\n",
      "Epoch [11/50], Step [60/71], Loss: 0.1206\n",
      "Epoch [11/50], Step [70/71], Loss: 0.1932\n",
      "Epoch [12/50], Step [10/71], Loss: 0.1265\n",
      "Epoch [12/50], Step [20/71], Loss: 0.1232\n",
      "Epoch [12/50], Step [30/71], Loss: 0.1309\n",
      "Epoch [12/50], Step [40/71], Loss: 0.1260\n",
      "Epoch [12/50], Step [50/71], Loss: 0.1770\n",
      "Epoch [12/50], Step [60/71], Loss: 0.1115\n",
      "Epoch [12/50], Step [70/71], Loss: 0.3027\n",
      "Epoch [13/50], Step [10/71], Loss: 0.0840\n",
      "Epoch [13/50], Step [20/71], Loss: 0.4557\n",
      "Epoch [13/50], Step [30/71], Loss: 0.1398\n",
      "Epoch [13/50], Step [40/71], Loss: 0.1604\n",
      "Epoch [13/50], Step [50/71], Loss: 0.2012\n",
      "Epoch [13/50], Step [60/71], Loss: 0.1866\n",
      "Epoch [13/50], Step [70/71], Loss: 0.1802\n",
      "Epoch [14/50], Step [10/71], Loss: 0.1304\n",
      "Epoch [14/50], Step [20/71], Loss: 0.1594\n",
      "Epoch [14/50], Step [30/71], Loss: 0.1114\n",
      "Epoch [14/50], Step [40/71], Loss: 0.2858\n",
      "Epoch [14/50], Step [50/71], Loss: 0.1927\n",
      "Epoch [14/50], Step [60/71], Loss: 0.3697\n",
      "Epoch [14/50], Step [70/71], Loss: 0.1038\n",
      "Epoch [15/50], Step [10/71], Loss: 0.3187\n",
      "Epoch [15/50], Step [20/71], Loss: 0.1785\n",
      "Epoch [15/50], Step [30/71], Loss: 0.2903\n",
      "Epoch [15/50], Step [40/71], Loss: 0.1527\n",
      "Epoch [15/50], Step [50/71], Loss: 0.2158\n",
      "Epoch [15/50], Step [60/71], Loss: 0.4129\n",
      "Epoch [15/50], Step [70/71], Loss: 0.1912\n",
      "Epoch [16/50], Step [10/71], Loss: 0.3730\n",
      "Epoch [16/50], Step [20/71], Loss: 0.2156\n",
      "Epoch [16/50], Step [30/71], Loss: 0.3174\n",
      "Epoch [16/50], Step [40/71], Loss: 0.1603\n",
      "Epoch [16/50], Step [50/71], Loss: 0.2194\n",
      "Epoch [16/50], Step [60/71], Loss: 0.2013\n",
      "Epoch [16/50], Step [70/71], Loss: 0.1034\n",
      "Epoch [17/50], Step [10/71], Loss: 0.2310\n",
      "Epoch [17/50], Step [20/71], Loss: 0.1454\n",
      "Epoch [17/50], Step [30/71], Loss: 0.1528\n",
      "Epoch [17/50], Step [40/71], Loss: 0.1815\n",
      "Epoch [17/50], Step [50/71], Loss: 0.0627\n",
      "Epoch [17/50], Step [60/71], Loss: 0.4515\n",
      "Epoch [17/50], Step [70/71], Loss: 0.2740\n",
      "Epoch [18/50], Step [10/71], Loss: 0.1545\n",
      "Epoch [18/50], Step [20/71], Loss: 0.1696\n",
      "Epoch [18/50], Step [30/71], Loss: 0.1559\n",
      "Epoch [18/50], Step [40/71], Loss: 0.1374\n",
      "Epoch [18/50], Step [50/71], Loss: 0.3507\n",
      "Epoch [18/50], Step [60/71], Loss: 0.1011\n",
      "Epoch [18/50], Step [70/71], Loss: 0.3546\n",
      "Epoch [19/50], Step [10/71], Loss: 0.2680\n",
      "Epoch [19/50], Step [20/71], Loss: 0.1589\n",
      "Epoch [19/50], Step [30/71], Loss: 0.2023\n",
      "Epoch [19/50], Step [40/71], Loss: 0.1761\n",
      "Epoch [19/50], Step [50/71], Loss: 0.1435\n",
      "Epoch [19/50], Step [60/71], Loss: 0.1903\n",
      "Epoch [19/50], Step [70/71], Loss: 0.1928\n",
      "Epoch [20/50], Step [10/71], Loss: 0.2513\n",
      "Epoch [20/50], Step [20/71], Loss: 0.0893\n",
      "Epoch [20/50], Step [30/71], Loss: 0.0936\n",
      "Epoch [20/50], Step [40/71], Loss: 0.1203\n",
      "Epoch [20/50], Step [50/71], Loss: 0.1748\n",
      "Epoch [20/50], Step [60/71], Loss: 0.1333\n",
      "Epoch [20/50], Step [70/71], Loss: 0.2080\n",
      "Epoch [21/50], Step [10/71], Loss: 0.0880\n",
      "Epoch [21/50], Step [20/71], Loss: 0.0845\n",
      "Epoch [21/50], Step [30/71], Loss: 0.2862\n",
      "Epoch [21/50], Step [40/71], Loss: 0.2025\n",
      "Epoch [21/50], Step [50/71], Loss: 0.1300\n",
      "Epoch [21/50], Step [60/71], Loss: 0.1578\n",
      "Epoch [21/50], Step [70/71], Loss: 0.1868\n",
      "Epoch [22/50], Step [10/71], Loss: 0.2441\n",
      "Epoch [22/50], Step [20/71], Loss: 0.0771\n",
      "Epoch [22/50], Step [30/71], Loss: 0.2463\n",
      "Epoch [22/50], Step [40/71], Loss: 0.1888\n",
      "Epoch [22/50], Step [50/71], Loss: 0.1509\n",
      "Epoch [22/50], Step [60/71], Loss: 0.1165\n",
      "Epoch [22/50], Step [70/71], Loss: 0.2100\n",
      "Epoch [23/50], Step [10/71], Loss: 0.1041\n",
      "Epoch [23/50], Step [20/71], Loss: 0.1978\n",
      "Epoch [23/50], Step [30/71], Loss: 0.1993\n",
      "Epoch [23/50], Step [40/71], Loss: 0.0970\n",
      "Epoch [23/50], Step [50/71], Loss: 0.1411\n",
      "Epoch [23/50], Step [60/71], Loss: 0.2088\n",
      "Epoch [23/50], Step [70/71], Loss: 0.4076\n",
      "Epoch [24/50], Step [10/71], Loss: 0.2448\n",
      "Epoch [24/50], Step [20/71], Loss: 0.1681\n",
      "Epoch [24/50], Step [30/71], Loss: 0.1863\n",
      "Epoch [24/50], Step [40/71], Loss: 0.2362\n",
      "Epoch [24/50], Step [50/71], Loss: 0.0599\n",
      "Epoch [24/50], Step [60/71], Loss: 0.1507\n",
      "Epoch [24/50], Step [70/71], Loss: 0.1913\n",
      "Epoch [25/50], Step [10/71], Loss: 0.2957\n",
      "Epoch [25/50], Step [20/71], Loss: 0.1069\n",
      "Epoch [25/50], Step [30/71], Loss: 0.2172\n",
      "Epoch [25/50], Step [40/71], Loss: 0.7275\n",
      "Epoch [25/50], Step [50/71], Loss: 0.1616\n",
      "Epoch [25/50], Step [60/71], Loss: 0.2026\n",
      "Epoch [25/50], Step [70/71], Loss: 0.1823\n",
      "Epoch [26/50], Step [10/71], Loss: 0.1241\n",
      "Epoch [26/50], Step [20/71], Loss: 0.3240\n",
      "Epoch [26/50], Step [30/71], Loss: 0.2279\n",
      "Epoch [26/50], Step [40/71], Loss: 0.2585\n",
      "Epoch [26/50], Step [50/71], Loss: 0.1529\n",
      "Epoch [26/50], Step [60/71], Loss: 0.1800\n",
      "Epoch [26/50], Step [70/71], Loss: 0.1796\n",
      "Epoch [27/50], Step [10/71], Loss: 0.1050\n",
      "Epoch [27/50], Step [20/71], Loss: 0.1167\n",
      "Epoch [27/50], Step [30/71], Loss: 0.2102\n",
      "Epoch [27/50], Step [40/71], Loss: 0.1213\n",
      "Epoch [27/50], Step [50/71], Loss: 0.1637\n",
      "Epoch [27/50], Step [60/71], Loss: 0.1042\n",
      "Epoch [27/50], Step [70/71], Loss: 0.1546\n",
      "Epoch [28/50], Step [10/71], Loss: 0.1109\n",
      "Epoch [28/50], Step [20/71], Loss: 0.1378\n",
      "Epoch [28/50], Step [30/71], Loss: 0.1111\n",
      "Epoch [28/50], Step [40/71], Loss: 0.1873\n",
      "Epoch [28/50], Step [50/71], Loss: 0.1004\n",
      "Epoch [28/50], Step [60/71], Loss: 0.1656\n",
      "Epoch [28/50], Step [70/71], Loss: 0.1535\n",
      "Epoch [29/50], Step [10/71], Loss: 0.2269\n",
      "Epoch [29/50], Step [20/71], Loss: 0.1461\n",
      "Epoch [29/50], Step [30/71], Loss: 0.0928\n",
      "Epoch [29/50], Step [40/71], Loss: 0.1223\n",
      "Epoch [29/50], Step [50/71], Loss: 0.0999\n",
      "Epoch [29/50], Step [60/71], Loss: 0.2065\n",
      "Epoch [29/50], Step [70/71], Loss: 0.2035\n",
      "Epoch [30/50], Step [10/71], Loss: 0.0983\n",
      "Epoch [30/50], Step [20/71], Loss: 0.2211\n",
      "Epoch [30/50], Step [30/71], Loss: 0.1054\n",
      "Epoch [30/50], Step [40/71], Loss: 0.1727\n",
      "Epoch [30/50], Step [50/71], Loss: 0.1675\n",
      "Epoch [30/50], Step [60/71], Loss: 0.1422\n",
      "Epoch [30/50], Step [70/71], Loss: 0.1567\n",
      "Epoch [31/50], Step [10/71], Loss: 0.0378\n",
      "Epoch [31/50], Step [20/71], Loss: 0.0670\n",
      "Epoch [31/50], Step [30/71], Loss: 0.1078\n",
      "Epoch [31/50], Step [40/71], Loss: 0.2035\n",
      "Epoch [31/50], Step [50/71], Loss: 0.1251\n",
      "Epoch [31/50], Step [60/71], Loss: 0.1782\n",
      "Epoch [31/50], Step [70/71], Loss: 0.1291\n",
      "Epoch [32/50], Step [10/71], Loss: 0.1023\n",
      "Epoch [32/50], Step [20/71], Loss: 0.1277\n",
      "Epoch [32/50], Step [30/71], Loss: 0.2427\n",
      "Epoch [32/50], Step [40/71], Loss: 0.1152\n",
      "Epoch [32/50], Step [50/71], Loss: 0.1238\n",
      "Epoch [32/50], Step [60/71], Loss: 0.2002\n",
      "Epoch [32/50], Step [70/71], Loss: 0.0972\n",
      "Epoch [33/50], Step [10/71], Loss: 0.1410\n",
      "Epoch [33/50], Step [20/71], Loss: 0.1650\n",
      "Epoch [33/50], Step [30/71], Loss: 0.1119\n",
      "Epoch [33/50], Step [40/71], Loss: 0.2675\n",
      "Epoch [33/50], Step [50/71], Loss: 0.0701\n",
      "Epoch [33/50], Step [60/71], Loss: 0.1491\n",
      "Epoch [33/50], Step [70/71], Loss: 0.1330\n",
      "Epoch [34/50], Step [10/71], Loss: 0.1069\n",
      "Epoch [34/50], Step [20/71], Loss: 0.1761\n",
      "Epoch [34/50], Step [30/71], Loss: 0.0871\n",
      "Epoch [34/50], Step [40/71], Loss: 0.2866\n",
      "Epoch [34/50], Step [50/71], Loss: 0.2044\n",
      "Epoch [34/50], Step [60/71], Loss: 0.3154\n",
      "Epoch [34/50], Step [70/71], Loss: 0.0897\n",
      "Epoch [35/50], Step [10/71], Loss: 0.2790\n",
      "Epoch [35/50], Step [20/71], Loss: 0.1206\n",
      "Epoch [35/50], Step [30/71], Loss: 0.1891\n",
      "Epoch [35/50], Step [40/71], Loss: 0.0973\n",
      "Epoch [35/50], Step [50/71], Loss: 0.2173\n",
      "Epoch [35/50], Step [60/71], Loss: 0.1401\n",
      "Epoch [35/50], Step [70/71], Loss: 0.0804\n",
      "Epoch [36/50], Step [10/71], Loss: 0.1916\n",
      "Epoch [36/50], Step [20/71], Loss: 0.1200\n",
      "Epoch [36/50], Step [30/71], Loss: 0.1408\n",
      "Epoch [36/50], Step [40/71], Loss: 0.2032\n",
      "Epoch [36/50], Step [50/71], Loss: 0.1888\n",
      "Epoch [36/50], Step [60/71], Loss: 0.1839\n",
      "Epoch [36/50], Step [70/71], Loss: 0.1786\n",
      "Epoch [37/50], Step [10/71], Loss: 0.1281\n",
      "Epoch [37/50], Step [20/71], Loss: 0.1321\n",
      "Epoch [37/50], Step [30/71], Loss: 0.0823\n",
      "Epoch [37/50], Step [40/71], Loss: 0.1599\n",
      "Epoch [37/50], Step [50/71], Loss: 0.1007\n",
      "Epoch [37/50], Step [60/71], Loss: 0.1029\n",
      "Epoch [37/50], Step [70/71], Loss: 0.2160\n",
      "Epoch [38/50], Step [10/71], Loss: 0.0912\n",
      "Epoch [38/50], Step [20/71], Loss: 0.0630\n",
      "Epoch [38/50], Step [30/71], Loss: 0.1994\n",
      "Epoch [38/50], Step [40/71], Loss: 0.1276\n",
      "Epoch [38/50], Step [50/71], Loss: 0.0752\n",
      "Epoch [38/50], Step [60/71], Loss: 0.5268\n",
      "Epoch [38/50], Step [70/71], Loss: 0.5713\n",
      "Epoch [39/50], Step [10/71], Loss: 0.9574\n",
      "Epoch [39/50], Step [20/71], Loss: 4.0447\n",
      "Epoch [39/50], Step [30/71], Loss: 1.3343\n",
      "Epoch [39/50], Step [40/71], Loss: 0.7127\n",
      "Epoch [39/50], Step [50/71], Loss: 0.0641\n",
      "Epoch [39/50], Step [60/71], Loss: 0.2045\n",
      "Epoch [39/50], Step [70/71], Loss: 0.2126\n",
      "Epoch [40/50], Step [10/71], Loss: 0.1951\n",
      "Epoch [40/50], Step [20/71], Loss: 0.3628\n",
      "Epoch [40/50], Step [30/71], Loss: 0.2329\n",
      "Epoch [40/50], Step [40/71], Loss: 0.0825\n",
      "Epoch [40/50], Step [50/71], Loss: 0.2543\n",
      "Epoch [40/50], Step [60/71], Loss: 0.1476\n",
      "Epoch [40/50], Step [70/71], Loss: 0.1700\n",
      "Epoch [41/50], Step [10/71], Loss: 0.1870\n",
      "Epoch [41/50], Step [20/71], Loss: 0.1074\n",
      "Epoch [41/50], Step [30/71], Loss: 0.0822\n",
      "Epoch [41/50], Step [40/71], Loss: 0.1671\n",
      "Epoch [41/50], Step [50/71], Loss: 0.1499\n",
      "Epoch [41/50], Step [60/71], Loss: 0.1486\n",
      "Epoch [41/50], Step [70/71], Loss: 0.0959\n",
      "Epoch [42/50], Step [10/71], Loss: 0.1013\n",
      "Epoch [42/50], Step [20/71], Loss: 0.1208\n",
      "Epoch [42/50], Step [30/71], Loss: 0.1966\n",
      "Epoch [42/50], Step [40/71], Loss: 0.1203\n",
      "Epoch [42/50], Step [50/71], Loss: 0.1081\n",
      "Epoch [42/50], Step [60/71], Loss: 0.0998\n",
      "Epoch [42/50], Step [70/71], Loss: 0.1241\n",
      "Epoch [43/50], Step [10/71], Loss: 0.1066\n",
      "Epoch [43/50], Step [20/71], Loss: 0.1510\n",
      "Epoch [43/50], Step [30/71], Loss: 0.0888\n",
      "Epoch [43/50], Step [40/71], Loss: 0.1142\n",
      "Epoch [43/50], Step [50/71], Loss: 0.1897\n",
      "Epoch [43/50], Step [60/71], Loss: 0.0711\n",
      "Epoch [43/50], Step [70/71], Loss: 0.1432\n",
      "Epoch [44/50], Step [10/71], Loss: 0.1141\n",
      "Epoch [44/50], Step [20/71], Loss: 0.0876\n",
      "Epoch [44/50], Step [30/71], Loss: 0.0831\n",
      "Epoch [44/50], Step [40/71], Loss: 0.1269\n",
      "Epoch [44/50], Step [50/71], Loss: 0.0930\n",
      "Epoch [44/50], Step [60/71], Loss: 0.0928\n",
      "Epoch [44/50], Step [70/71], Loss: 0.1034\n",
      "Epoch [45/50], Step [10/71], Loss: 0.0847\n",
      "Epoch [45/50], Step [20/71], Loss: 0.1408\n",
      "Epoch [45/50], Step [30/71], Loss: 0.1682\n",
      "Epoch [45/50], Step [40/71], Loss: 0.1159\n",
      "Epoch [45/50], Step [50/71], Loss: 0.0791\n",
      "Epoch [45/50], Step [60/71], Loss: 0.1246\n",
      "Epoch [45/50], Step [70/71], Loss: 0.1332\n",
      "Epoch [46/50], Step [10/71], Loss: 0.0732\n",
      "Epoch [46/50], Step [20/71], Loss: 0.0906\n",
      "Epoch [46/50], Step [30/71], Loss: 0.1650\n",
      "Epoch [46/50], Step [40/71], Loss: 0.1086\n",
      "Epoch [46/50], Step [50/71], Loss: 0.1269\n",
      "Epoch [46/50], Step [60/71], Loss: 0.0817\n",
      "Epoch [46/50], Step [70/71], Loss: 0.1624\n",
      "Epoch [47/50], Step [10/71], Loss: 0.0905\n",
      "Epoch [47/50], Step [20/71], Loss: 0.1186\n",
      "Epoch [47/50], Step [30/71], Loss: 0.1567\n",
      "Epoch [47/50], Step [40/71], Loss: 0.1902\n",
      "Epoch [47/50], Step [50/71], Loss: 0.0930\n",
      "Epoch [47/50], Step [60/71], Loss: 0.1478\n",
      "Epoch [47/50], Step [70/71], Loss: 0.1283\n",
      "Epoch [48/50], Step [10/71], Loss: 0.0922\n",
      "Epoch [48/50], Step [20/71], Loss: 0.0610\n",
      "Epoch [48/50], Step [30/71], Loss: 0.1057\n",
      "Epoch [48/50], Step [40/71], Loss: 0.0639\n",
      "Epoch [48/50], Step [50/71], Loss: 0.2503\n",
      "Epoch [48/50], Step [60/71], Loss: 0.1208\n",
      "Epoch [48/50], Step [70/71], Loss: 0.1249\n",
      "Epoch [49/50], Step [10/71], Loss: 0.0925\n",
      "Epoch [49/50], Step [20/71], Loss: 0.1434\n",
      "Epoch [49/50], Step [30/71], Loss: 0.1085\n",
      "Epoch [49/50], Step [40/71], Loss: 0.0909\n",
      "Epoch [49/50], Step [50/71], Loss: 0.1352\n",
      "Epoch [49/50], Step [60/71], Loss: 0.1057\n",
      "Epoch [49/50], Step [70/71], Loss: 0.1432\n",
      "Epoch [50/50], Step [10/71], Loss: 0.0799\n",
      "Epoch [50/50], Step [20/71], Loss: 0.0965\n",
      "Epoch [50/50], Step [30/71], Loss: 0.1878\n",
      "Epoch [50/50], Step [40/71], Loss: 0.1750\n",
      "Epoch [50/50], Step [50/71], Loss: 0.1525\n",
      "Epoch [50/50], Step [60/71], Loss: 0.1438\n",
      "Epoch [50/50], Step [70/71], Loss: 0.1569\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_siamese_network(siamese_net, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(siamese_net.state_dict(), 'siamese_net_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "siamese_net.load_state_dict(torch.load('siamese_net.pth'))\n",
    "trained_model = siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.4859\n",
      "Test F1 Score: 0.1205\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_siamese_network(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in enumerate(test_loader):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            # Calculate the euclidean distance between the outputs\n",
    "            dist = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Get predictions\n",
    "            predicted = (dist < 1.0).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "            # print(f\"Dist: {dist}, Predicted: {predicted}, Actual: {labels}\")\n",
    "\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "\n",
    "    # accuracy:\n",
    "    accuracy = correct / total\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # move to cpu:\n",
    "    all_labels = [label.cpu().numpy() for label in all_labels]\n",
    "    all_predictions = [prediction.cpu().numpy() for prediction in all_predictions]\n",
    "\n",
    "    # f1 score:\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # confusion matrix:\n",
    "    matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "test_siamese_network(trained_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_test = eval_data.sample(1)\n",
    "# img1, img2, label = random_test.iloc[0]\n",
    "# img1 = img1.strip(\"()\")\n",
    "# img2 = img2.strip(\"()\")\n",
    "\n",
    "# # Load images\n",
    "# image1 = Image.open(img1).convert(\"RGB\")\n",
    "# image2 = Image.open(img2).convert(\"RGB\")\n",
    "\n",
    "# # Print Images\n",
    "# display(image1)\n",
    "# display(image2)\n",
    "\n",
    "# # Apply transformations\n",
    "# image1 = transform(image1).unsqueeze(0)\n",
    "# image2 = transform(image2).unsqueeze(0)\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = trained_model(image1, image2)\n",
    "\n",
    "# # Calculate the euclidean distance between the outputs\n",
    "# # diff = output1 - output2\n",
    "# # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "# # dist = torch.sqrt(dist_sq)\n",
    "# dist = F.pairwise_distance(output1, output2)\n",
    "# print(f\"Distance: {dist.item()}\")\n",
    "\n",
    "# # Get predictions\n",
    "# predicted = (dist < 1.0).float()\n",
    "# true_label = 0 if label < 2 else 1\n",
    "\n",
    "# print(f\"Predicted: {predicted.item()}, Actual: {true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca of the output:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the output of the model\n",
    "outputs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img1, img2, label in eval_loader:\n",
    "        output1, output2 = trained_model(img1, img2)\n",
    "        outputs.append(output1)\n",
    "        labels.append(label)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_outputs = pca.fit_transform(outputs)\n",
    "\n",
    "# Plot the PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_outputs[:, 0], y=pca_outputs[:, 1], hue=labels, palette='viridis')\n",
    "plt.title('PCA of the Siamese Network Output')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pt\")\n",
    "# print(\"Model Saved Successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_rows = full_data_paths[full_data_paths['similarity'].isna()].sample(5000)\n",
    "unlabeled_rows = unlabeled_rows[['image1_path', 'image2_path', 'similarity']]\n",
    "unlabeled_dataset = ImageSimilarityDataset(unlabeled_rows, transform=transform)\n",
    "data_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def active_learning_confident_samples(model, dataloader, margin=1.0, budget=100):\n",
    "    \"\"\"\n",
    "    Identify the least confident samples from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Siamese network.\n",
    "        dataloader: DataLoader for the dataset you want to evaluate.\n",
    "        margin: The margin used in the contrastive loss.\n",
    "        top_k: Number of least confident samples to return.\n",
    "    \n",
    "    Returns:\n",
    "        A list of the top_k least confident samples (input pairs and distances).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    least_confident_samples = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for img1_path, img2_path, img1, img2, labels in tqdm(dataloader):\n",
    "            # Get the model outputs for both images\n",
    "            output1, output2 = model(img1, img2)\n",
    "            \n",
    "            # Calculate pairwise distance\n",
    "            distances = F.pairwise_distance(output1, output2)\n",
    "            \n",
    "            # Calculate confidence score (distance from the margin)\n",
    "            confidence_scores = torch.abs(distances - margin)\n",
    "\n",
    "            # Collect the least confident samples (small confidence score means high uncertainty)\n",
    "            for i in range(len(confidence_scores)):\n",
    "                least_confident_samples.append((img1_path[i], img2_path[i], distances[i].item(), confidence_scores[i].item()))\n",
    "\n",
    "    # Sort samples by confidence score (ascending, to get least confident samples)\n",
    "    least_confident_samples.sort(key=lambda x: x[3])\n",
    "\n",
    "    # Return the top_k least confident samples\n",
    "    return least_confident_samples[:budget]\n",
    "\n",
    "least_confident_samples = active_learning_confident_samples(trained_model, data_loader, margin=1.0, budget=100)\n",
    "print(least_confident_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from confidence samples save into a csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
