{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Similiarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of choosing the similarity function that determines the images that are similar to a given image.\n",
    "The implementation of the network is based on the Siamese network implementation. This class of networks is known to be more robust to class imbalance, so it fits the data on which we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         image1_path  \\\n",
      "0  (datasets/house_styles/all_images/001_d2c7428a...   \n",
      "1  (datasets/house_styles/all_images/453_d7b5d246...   \n",
      "2  (datasets/house_styles/all_images/116_32f01ef6...   \n",
      "3  (datasets/house_styles/all_images/301_b73b9663...   \n",
      "4  (datasets/house_styles/all_images/042_06b56791...   \n",
      "\n",
      "                                         image2_path  similarity  \n",
      "0  (datasets/house_styles/all_images/366_08eff319...         3.0  \n",
      "1  (datasets/house_styles/all_images/122_e44a0cb3...         0.0  \n",
      "2  (datasets/house_styles/all_images/174_55a7b3f9...         0.0  \n",
      "3  (datasets/house_styles/all_images/116_32f01ef6...         0.0  \n",
      "4  (datasets/house_styles/all_images/069_d3bedc1f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "base_path = 'active_learning_labels/'\n",
    "full_data_paths = pd.read_csv(base_path + 'round_0.csv')\n",
    "\n",
    "current_round = 1\n",
    "\n",
    "if current_round > 0:\n",
    "    for i in range(1, current_round):\n",
    "        path = base_path + 'round_' + str(i) + '.csv'\n",
    "        data = pd.read_csv(path)\n",
    "        full_data_paths = pd.concat([full_data_paths, data], ignore_index=True)\n",
    "\n",
    "data_paths = full_data_paths[['image1_path', 'image2_path', 'similarity']]\n",
    "\n",
    "print(data_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (120, 3) Test:  (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "train_data = data_paths.sample(frac=0.8, random_state=42)\n",
    "eval_data = data_paths.drop(train_data.index)\n",
    "\n",
    "print(\"Train: \", train_data.shape, \"Eval: \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.master_path = ''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from the paths\n",
    "        image1_path = self.master_path + self.data.iloc[idx, 0].strip(\"()\")\n",
    "        image2_path = self.master_path + self.data.iloc[idx, 1].strip(\"()\")\n",
    "        \n",
    "        # Load images\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        \n",
    "        # Get similarity score\n",
    "        similarity = self.data.iloc[idx, 2]\n",
    "        label = 0 if similarity < 2 else 1\n",
    "        \n",
    "        return image1_path, image2_path, image1, image2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define transformations (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 224x224 (standard for many models)\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained models\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling layer to ensure the output size is consistent\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        x = self.cnn1(x)\n",
    "        x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # Forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Setting up the Sequential of CNN Layers\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=1),  # Adjusted for RGB input\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to ensure the output size is consistent\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "#         # Defining the fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(1024, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "    #     x = self.fc1(x)\n",
    "    #     return x\n",
    "\n",
    "    # def forward(self, input1, input2):\n",
    "    #     # Forward pass of input 1\n",
    "    #     output1 = self.forward_once(input1)\n",
    "    #     # Forward pass of input 2\n",
    "    #     output2 = self.forward_once(input2)\n",
    "    #     return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallSiameseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SmallSiameseNetwork, self).__init__()\n",
    "        # Setting up a smaller CNN\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=1),  # Fewer filters, smaller kernel size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # Max pooling with smaller stride to reduce spatial dimensions\n",
    "            nn.Dropout2d(p=0.2),  # Reduced dropout rate\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),  # Fewer filters\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # Smaller max-pooling\n",
    "            nn.Dropout2d(p=0.2),\n",
    "\n",
    "            # nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling layer to standardize output size (reduce to 3x3)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))  # Smaller output size (3x3)\n",
    "        \n",
    "        # Defining smaller fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * 3 * 3, 128),  # Reduced size based on new feature map size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),  # Keep some dropout for regularization\n",
    "            \n",
    "            nn.Linear(128, 64),  # Smaller fully connected layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(64, 2)  # Output layer remains the same\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        x = self.cnn1(x)\n",
    "        x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # Forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        ### Binary:\n",
    "        # euclidian distance\n",
    "        diff = x0 - x1\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "\n",
    "        mdist = self.margin - dist\n",
    "        dist = torch.clamp(mdist, min=0.0)\n",
    "\n",
    "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_network(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the appropriate device\n",
    "            # img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print statistics\n",
    "            if (i + 1) % 5 == 0:  # Print every 5 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [5/15], Loss: 0.1588\n",
      "Epoch [1/10], Step [10/15], Loss: 0.0906\n",
      "Epoch [1/10], Step [15/15], Loss: 0.1079\n",
      "Epoch [2/10], Step [5/15], Loss: 0.0731\n",
      "Epoch [2/10], Step [10/15], Loss: 0.0686\n",
      "Epoch [2/10], Step [15/15], Loss: 0.0999\n",
      "Epoch [3/10], Step [5/15], Loss: 0.0832\n",
      "Epoch [3/10], Step [10/15], Loss: 0.0772\n",
      "Epoch [3/10], Step [15/15], Loss: 0.0897\n",
      "Epoch [4/10], Step [5/15], Loss: 0.0817\n",
      "Epoch [4/10], Step [10/15], Loss: 0.0771\n",
      "Epoch [4/10], Step [15/15], Loss: 0.0758\n",
      "Epoch [5/10], Step [5/15], Loss: 0.1165\n",
      "Epoch [5/10], Step [10/15], Loss: 0.0861\n",
      "Epoch [5/10], Step [15/15], Loss: 0.0729\n",
      "Epoch [6/10], Step [5/15], Loss: 0.0583\n",
      "Epoch [6/10], Step [10/15], Loss: 0.0907\n",
      "Epoch [6/10], Step [15/15], Loss: 0.0688\n",
      "Epoch [7/10], Step [5/15], Loss: 0.0823\n",
      "Epoch [7/10], Step [10/15], Loss: 0.0751\n",
      "Epoch [7/10], Step [15/15], Loss: 0.0464\n",
      "Epoch [8/10], Step [5/15], Loss: 0.0712\n",
      "Epoch [8/10], Step [10/15], Loss: 0.0755\n",
      "Epoch [8/10], Step [15/15], Loss: 0.0645\n",
      "Epoch [9/10], Step [5/15], Loss: 0.0795\n",
      "Epoch [9/10], Step [10/15], Loss: 0.0858\n",
      "Epoch [9/10], Step [15/15], Loss: 0.0971\n",
      "Epoch [10/10], Step [5/15], Loss: 0.0523\n",
      "Epoch [10/10], Step [10/15], Loss: 0.1083\n",
      "Epoch [10/10], Step [15/15], Loss: 0.0718\n"
     ]
    }
   ],
   "source": [
    "siamese_net = SmallSiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageSimilarityDataset(train_data, transform=transform)\n",
    "eval_dataset = ImageSimilarityDataset(eval_data, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "\n",
    "trained_model = train_siamese_network(siamese_net, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def test_siamese_network(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1_path, img2_path, img1, img2, labels in test_loader:\n",
    "            # Move tensors to the appropriate device\n",
    "            # img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            # Calculate the euclidean distance between the outputs\n",
    "            dist = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Get predictions\n",
    "            predicted = (dist < 1.0).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # confusion matrix:\n",
    "    matrix = confusion_matrix(labels, predicted)\n",
    "    # plot:\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "test_siamese_network(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_test = eval_data.sample(1)\n",
    "# img1, img2, label = random_test.iloc[0]\n",
    "# img1 = img1.strip(\"()\")\n",
    "# img2 = img2.strip(\"()\")\n",
    "\n",
    "# # Load images\n",
    "# image1 = Image.open(img1).convert(\"RGB\")\n",
    "# image2 = Image.open(img2).convert(\"RGB\")\n",
    "\n",
    "# # Print Images\n",
    "# display(image1)\n",
    "# display(image2)\n",
    "\n",
    "# # Apply transformations\n",
    "# image1 = transform(image1).unsqueeze(0)\n",
    "# image2 = transform(image2).unsqueeze(0)\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = trained_model(image1, image2)\n",
    "\n",
    "# # Calculate the euclidean distance between the outputs\n",
    "# # diff = output1 - output2\n",
    "# # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "# # dist = torch.sqrt(dist_sq)\n",
    "# dist = F.pairwise_distance(output1, output2)\n",
    "# print(f\"Distance: {dist.item()}\")\n",
    "\n",
    "# # Get predictions\n",
    "# predicted = (dist < 1.0).float()\n",
    "# true_label = 0 if label < 2 else 1\n",
    "\n",
    "# print(f\"Predicted: {predicted.item()}, Actual: {true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca of the output:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the output of the model\n",
    "outputs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img1, img2, label in eval_loader:\n",
    "        output1, output2 = trained_model(img1, img2)\n",
    "        outputs.append(output1)\n",
    "        labels.append(label)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_outputs = pca.fit_transform(outputs)\n",
    "\n",
    "# Plot the PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_outputs[:, 0], y=pca_outputs[:, 1], hue=labels, palette='viridis')\n",
    "plt.title('PCA of the Siamese Network Output')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pt\")\n",
    "# print(\"Model Saved Successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_rows = full_data_paths[full_data_paths['similarity'].isna()].sample(5000)\n",
    "unlabeled_rows = unlabeled_rows[['image1_path', 'image2_path', 'similarity']]\n",
    "unlabeled_dataset = ImageSimilarityDataset(unlabeled_rows, transform=transform)\n",
    "data_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def active_learning_confident_samples(model, dataloader, margin=1.0, budget=100):\n",
    "    \"\"\"\n",
    "    Identify the least confident samples from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Siamese network.\n",
    "        dataloader: DataLoader for the dataset you want to evaluate.\n",
    "        margin: The margin used in the contrastive loss.\n",
    "        top_k: Number of least confident samples to return.\n",
    "    \n",
    "    Returns:\n",
    "        A list of the top_k least confident samples (input pairs and distances).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    least_confident_samples = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for img1_path, img2_path, img1, img2, labels in tqdm(dataloader):\n",
    "            # Get the model outputs for both images\n",
    "            output1, output2 = model(img1, img2)\n",
    "            \n",
    "            # Calculate pairwise distance\n",
    "            distances = F.pairwise_distance(output1, output2)\n",
    "            \n",
    "            # Calculate confidence score (distance from the margin)\n",
    "            confidence_scores = torch.abs(distances - margin)\n",
    "\n",
    "            # Collect the least confident samples (small confidence score means high uncertainty)\n",
    "            for i in range(len(confidence_scores)):\n",
    "                least_confident_samples.append((img1_path[i], img2_path[i], distances[i].item(), confidence_scores[i].item()))\n",
    "\n",
    "    # Sort samples by confidence score (ascending, to get least confident samples)\n",
    "    least_confident_samples.sort(key=lambda x: x[3])\n",
    "\n",
    "    # Return the top_k least confident samples\n",
    "    return least_confident_samples[:budget]\n",
    "\n",
    "least_confident_samples = active_learning_confident_samples(trained_model, data_loader, margin=1.0, budget=100)\n",
    "print(least_confident_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from confidence samples save into a csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
