{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate-Nearest-Neighbors Image Search Pipeline\n",
    "\n",
    "This notebook details a simple pipeline for performing approximate nearest neighbors search on a dataset of images by tokenizing the images into vectors using a pre-trained model and then using a standard library for approximate nearest neighbors search. The metric used for the search is cosine similarity.\n",
    "\n",
    "We consider the following implementation a baseline for image search results, utilizing simple yet powerful tools to achieve a good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <CDAC6E34-8608-3E70-8B2F-32BCD38E90FB> /Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the folder paths\n",
    "dataset_path = \"datasets/house_styles\"\n",
    "image_folder = \"datasets/house_styles/all_images\"\n",
    "label_file = \"datasets/house_styles/labels.csv\"\n",
    "split_file = \"datasets/house_styles/split_mask.csv\"\n",
    "\n",
    "# Output tokenized database\n",
    "output_index_file = \"vector_dbs/house_styles/image_vectors_index.npy\"\n",
    "output_query_file = \"vector_dbs/house_styles/image_vectors_query.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Computer Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CLIP model and processor from Hugging Face\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set up the image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train - Test Split: Run Only Once ###\n",
    "\n",
    "image_names = [f for f in os.listdir(image_folder)]\n",
    "print(f\"Found {len(image_names)} images in '{image_folder}'.\")\n",
    "\n",
    "random_seed = 42\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Randomly split the image names into training and testing sets\n",
    "train_names, test_names = train_test_split(\n",
    "    image_names,\n",
    "    train_size=train_ratio,\n",
    "    random_state=random_seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create a DataFrame to store image names and their corresponding split\n",
    "split_df = pd.DataFrame({\n",
    "    'image_name': image_names,\n",
    "    'split': ['train' if name in train_names else 'test' for name in image_names]\n",
    "})\n",
    "\n",
    "# Save the split information to a CSV file\n",
    "split_df.to_csv(split_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of index images to process: 693\n",
      "Number of query images to process: 174\n"
     ]
    }
   ],
   "source": [
    "# Load the split information from the CSV file\n",
    "split_df = pd.read_csv(split_file)\n",
    "\n",
    "# Extract training image names\n",
    "index_image_names = split_df[split_df['split'] == 'train']['image_name'].tolist()\n",
    "query_image_names = split_df[split_df['split'] == 'test']['image_name'].tolist()\n",
    "all_image_names = index_image_names + query_image_names\n",
    "\n",
    "print(f\"Number of index images to process: {len(index_image_names)}\")\n",
    "print(f\"Number of query images to process: {len(query_image_names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return transform(image)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error loading image '{image_path}': {e}\")\n",
    "\n",
    "# Function to tokenize and get vector representation of an image\n",
    "def get_image_vector(image_tensor):\n",
    "    # Normalize after conversion to avoid PIL conversion issues\n",
    "    # normalized_tensor = transforms.Normalize(\n",
    "    #     mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "    #     std=(0.26862954, 0.26130258, 0.27577711)\n",
    "    # )(image_tensor)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    inputs = processor(images=image_tensor, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "def save_image_vectors(image_names, output_file):\n",
    "    image_vectors = []\n",
    "    for image_name in tqdm(image_names):\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "        try:\n",
    "            image_tensor = load_and_preprocess_image(image_path)\n",
    "            image_vector = get_image_vector(image_tensor)\n",
    "            image_vectors.append(image_vector)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image '{image_path}': {e}\")\n",
    "\n",
    "    image_vectors = np.stack(image_vectors)\n",
    "    np.save(output_file, image_vectors)\n",
    "    return image_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [00:48<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693 image vectors saved to 'vector_dbs/house_styles/image_vectors_index.npy'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:11<00:00, 15.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 image vectors saved to 'vector_dbs/house_styles/image_vectors_query.npy'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_vectors = save_image_vectors(index_image_names, output_index_file)\n",
    "print(f\"{len(index_vectors)} image vectors saved to '{output_index_file}'.\")\n",
    "\n",
    "query_vectors = save_image_vectors(query_image_names, output_query_file)\n",
    "print(f\"{len(query_vectors)} image vectors saved to '{output_query_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 693 index vectors and 174 query vectors.\n"
     ]
    }
   ],
   "source": [
    "index_vectors = np.load(output_index_file)\n",
    "query_vectors = np.load(output_query_file)\n",
    "\n",
    "print(f\"Loaded {len(index_vectors)} index vectors and {len(query_vectors)} query vectors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_optimized_exhaustive_search(\n",
    "        index_vectors: np.ndarray,\n",
    "        query_vectors: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function performs an optimized exhaustive search.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "        dim: The dimensionality of the vectors.\n",
    "    Returns:\n",
    "        An array of shape (n_queries, k) containing the indices of the k nearest neighbors for each query vector.\n",
    "    \"\"\"\n",
    "    ann_lists = []\n",
    "    for query_vec in tqdm(query_vectors):\n",
    "        # distances = np.linalg.norm(index_vectors - query_vec, axis=1)\n",
    "        distances = np.dot(index_vectors, query_vec)\n",
    "        ann_lists.append(list(np.argsort(distances)[:k]))\n",
    "    return np.array(ann_lists)\n",
    "\n",
    "def compute_recall_at_k(\n",
    "        nn_gt: np.ndarray,\n",
    "        ann: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the recall@k.\n",
    "    Args:\n",
    "        nn_gt: The ground truth nearest neighbors.\n",
    "        ann: The approximate nearest neighbors.\n",
    "        k: The number of nearest neighbors to consider.\n",
    "    Returns:\n",
    "        The recall@k.\n",
    "    \"\"\"\n",
    "    return round(sum([len(set(ann[i]) & set(nn_gt[i])) / k for i in range(len(ann))])/len(ann), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_lsh_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "        nbits: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss LSH index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "        nbits: The number of bits to use in the hash.\n",
    "    Returns:\n",
    "        A Faiss LSH index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexLSH(dim, nbits)\n",
    "    index.add(index_vectors)\n",
    "    return index\n",
    "\n",
    "def build_faiss_flatl2_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss flat L2 index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "    Returns:\n",
    "        A Faiss flat L2 index.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    num_vectors = index_vectors.shape[0]\n",
    "    dim = index_vectors.shape[1]\n",
    "    norm_index_vectors = index_vectors / np.linalg.norm(index_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(norm_index_vectors)\n",
    "    return index\n",
    "\n",
    "def faiss_search(\n",
    "        query_vectors: np.ndarray,\n",
    "        index: faiss.Index,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function uses a Faiss index to search for the k-nearest neighbors of query_vectors.\n",
    "    Args:\n",
    "        query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "        index: A Faiss index.\n",
    "        k: The number of nearest neighbors to retrieve.\n",
    "    Returns:\n",
    "        An array of shape (, ) containing the indices of the k-nearest neighbors for each query vector.\n",
    "    \"\"\"\n",
    "    if not isinstance(index, faiss.Index):\n",
    "        raise ValueError(\"The index must be a Faiss index.\")\n",
    "    if isinstance(index, faiss.IndexFlatL2):\n",
    "        num_queries = query_vectors.shape[0]\n",
    "        dim = query_vectors.shape[1]\n",
    "        query_vectors = query_vectors / np.linalg.norm(query_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    distances, indices = index.search(query_vectors, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "dim = index_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "faiss_lsh_index = build_faiss_lsh_index(index_vectors, dim, nbits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 999 µs, sys: 1.27 ms, total: 2.27 ms\n",
      "Wall time: 2.22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_l2_index = build_faiss_flatl2_index(index_vectors, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Index Recall Compared to Exact Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exhaustive Search - Comparative Baseline ###\n",
    "%%time\n",
    "gt_nn = semi_optimized_exhaustive_search(index_vectors, query_vectors, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Faiss LSH Search ###\n",
    "faiss_lsh_ann = faiss_search(query_vectors, faiss_lsh_index, k)\n",
    "\n",
    "print(f\"recall@10 for faiss_lsh_index: {compute_recall_at_k(gt_nn, faiss_lsh_ann, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### Faiss L2 Search ###\n",
    "faiss_l2_ann = faiss_search(query_vectors, faiss_l2_index, k)\n",
    "\n",
    "#  print(f\"recall@10 for faiss_l2_index: {compute_recall_at_k(gt_nn, faiss_l2_ann, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "For vectors resulting from CLIP embeddings, we see the best performance in ANN compared to exact search in the -- Index. With a recall of --.\n",
    "\n",
    "We will use this index in the rest of out work and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMSLIB Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/naomi/miniconda3/envs/Lab2_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - nmslib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.8.30  |       hf0a4a13_0         155 KB  conda-forge\n",
      "    certifi-2024.7.4           |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    nmslib-2.1.1               |  py310hb6aeb05_0         592 KB\n",
      "    pybind11-2.13.5            |  py310h7306fd8_0         189 KB  conda-forge\n",
      "    pybind11-global-2.13.5     |  py310h7306fd8_0         177 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  nmslib             pkgs/main/osx-arm64::nmslib-2.1.1-py310hb6aeb05_0 \n",
      "  pybind11           conda-forge/osx-arm64::pybind11-2.13.5-py310h7306fd8_0 \n",
      "  pybind11-global    conda-forge/osx-arm64::pybind11-global-2.13.5-py310h7306fd8_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2024.7.2-h~ --> conda-forge::ca-certificates-2024.8.30-hf0a4a13_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.7.4~ --> conda-forge/noarch::certifi-2024.7.4-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "nmslib-2.1.1         | 592 KB    |                                       |   0% \n",
      "pybind11-2.13.5      | 189 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "pybind11-global-2.13 | 177 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    | ##############################6       |  83% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pybind11-2.13.5      | 189 KB    | ###1                                  |   8% \u001b[A\n",
      "\n",
      "nmslib-2.1.1         | 592 KB    | 9                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "pybind11-global-2.13 | 177 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "pybind11-2.13.5      | 189 KB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************"
     ]
    }
   ],
   "source": [
    "import nmslib\n",
    "\n",
    "# Example data\n",
    "data = np.random.random((1000, 128)).astype(np.float32)\n",
    "\n",
    "# Create an index using a custom distance function\n",
    "def custom_distance(vec1, vec2):\n",
    "    return np.sum(np.abs(vec1 - vec2))  # Example: Manhattan distance\n",
    "\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(data)\n",
    "index.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "# Query\n",
    "ids, distances = index.knnQuery(data[0], k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyNNDescent Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run this in Lab2_env:\n",
    "# conda install -c conda-forge pynndescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynndescent\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a unique distance function and compile it with Numba:\n",
    "@njit\n",
    "def custom_distance(x, y):\n",
    "    return np.sum(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pynndescent.NNDescent(index_vectors, metric=custom_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, distances = index.query(query_vectors, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:00<00:00, 4104.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive search time: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Exhaustive Search - Comparative Baseline ###\n",
    "start = time.time()\n",
    "gt_nn = semi_optimized_exhaustive_search(index_vectors, query_vectors, k)\n",
    "end = time.time()\n",
    "print(f\"Exhaustive search time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nndescent_index = pynndescent.NNDescent(index_vectors, metric='dot')\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query time: 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "indices, distances = nndescent_index.query(query_vectors, k)\n",
    "end = time.time()\n",
    "print(f\"Query time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20 for pynndescent: 0.02\n"
     ]
    }
   ],
   "source": [
    "recall = compute_recall_at_k(gt_nn, indices, k)\n",
    "print(f\"recall@{k} for pynndescent: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
