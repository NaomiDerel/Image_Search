{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Similiarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of choosing the similarity function that determines the images that are similar to a given image.\n",
    "The implementation of the network is based on the Siamese network implementation. This class of networks is known to be more robust to class imbalance, so it fits the data on which we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         image1_path  \\\n",
      "0  (datasets/house_styles/all_images/001_d2c7428a...   \n",
      "1  (datasets/house_styles/all_images/453_d7b5d246...   \n",
      "2  (datasets/house_styles/all_images/116_32f01ef6...   \n",
      "3  (datasets/house_styles/all_images/301_b73b9663...   \n",
      "4  (datasets/house_styles/all_images/042_06b56791...   \n",
      "\n",
      "                                         image2_path  similarity  \n",
      "0  (datasets/house_styles/all_images/366_08eff319...         3.0  \n",
      "1  (datasets/house_styles/all_images/122_e44a0cb3...         0.0  \n",
      "2  (datasets/house_styles/all_images/174_55a7b3f9...         0.0  \n",
      "3  (datasets/house_styles/all_images/116_32f01ef6...         0.0  \n",
      "4  (datasets/house_styles/all_images/069_d3bedc1f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "base_path = 'active_learning_labels/'\n",
    "full_data_paths = pd.read_csv(base_path + 'round_0.csv')\n",
    "\n",
    "current_round = 1\n",
    "\n",
    "if current_round > 0:\n",
    "    for i in range(1, current_round):\n",
    "        path = base_path + 'round_' + str(i) + '.csv'\n",
    "        data = pd.read_csv(path)\n",
    "        full_data_paths = pd.concat([full_data_paths, data], ignore_index=True)\n",
    "\n",
    "data_paths = full_data_paths[['image1_path', 'image2_path', 'similarity']]\n",
    "\n",
    "print(data_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (566, 3) Eval:  (142, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "train_data = data_paths.sample(frac=0.8, random_state=42)\n",
    "eval_data = data_paths.drop(train_data.index)\n",
    "\n",
    "print(\"Train: \", train_data.shape, \"Eval: \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Computer Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained CLIP model and processor from Hugging Face\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set up the image transformation pipeline\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Define the dataset class\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomApply([transforms.RandomResizedCrop(224)], p=0.2),  # 20% chance of random resized crop\n",
    "    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.2),  # 20% chance of horizontal flip\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.2),  # 20% chance of color jitter\n",
    "    transforms.ToTensor(),  # Always apply ToTensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=clip_transform, augmentations=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        self.master_path = ''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from the paths\n",
    "        image1_path = self.master_path + self.data.iloc[idx, 0].strip(\"()\")\n",
    "        image2_path = self.master_path + self.data.iloc[idx, 1].strip(\"()\")\n",
    "        \n",
    "        # Load images\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentations:\n",
    "            image1 = self.augmentations(image1)\n",
    "            image2 = self.augmentations(image2)\n",
    "\n",
    "        # CLIP:\n",
    "        images_features = []\n",
    "        for img in [image1, image2]:\n",
    "            image_tensor = self.transform(img).unsqueeze(0)\n",
    "            inputs = processor(images=image_tensor, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "                #images_features.append(image_features.numpy().flatten())\n",
    "            images_features.append(image_features.squeeze())  # Ensure it's a 512-dimensional tensor\n",
    "\n",
    "        \n",
    "        # # Apply transforms if provided\n",
    "        # if self.transform:\n",
    "        #     image1 = self.transform(image1)\n",
    "        #     image2 = self.transform(image2)\n",
    "        \n",
    "        # Get similarity score\n",
    "        similarity = self.data.iloc[idx, 2]\n",
    "        label = 0 if similarity < 3 else 1\n",
    "        \n",
    "        return image1_path, image2_path, images_features[0], images_features[1], torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageSimilarityDataset(train_data, transform=clip_transform, augmentations=None)\n",
    "eval_dataset = ImageSimilarityDataset(eval_data, transform=clip_transform, augmentations=None)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)  # Keep larger dimension here\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Using fully connected layers to process 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # First layer to reduce dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),  # Second layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)     # Output layer with 2 units (for similarity comparison)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         return self.fc1(x)\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass for both inputs\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Use fully connected layers for processing 512-dimensional CLIP features\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  # Assuming CLIP outputs 512-dimensional features\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass through fully connected layers\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Setting up the Sequential of CNN Layers\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "            \n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to ensure the output size is consistent\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "#         # Defining the fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(1024, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n",
    "\n",
    "# # class SiameseNetwork(nn.Module):\n",
    "    \n",
    "# #     def __init__(self):\n",
    "# #         super(SiameseNetwork, self).__init__()\n",
    "# #         # Setting up the Sequential of CNN Layers\n",
    "# #         self.cnn1 = nn.Sequential(\n",
    "# #             nn.Conv2d(3, 96, kernel_size=11, stride=1),  # Adjusted for RGB input\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "# #             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "\n",
    "# #             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.MaxPool2d(3, stride=2),\n",
    "# #             nn.Dropout2d(p=0.3),\n",
    "# #         )\n",
    "        \n",
    "# #         # Adaptive pooling layer to ensure the output size is consistent\n",
    "# #         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "# #         # Defining the fully connected layers\n",
    "# #         self.fc1 = nn.Sequential(\n",
    "# #             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "# #             nn.ReLU(inplace=True),\n",
    "# #             nn.Dropout(p=0.5),\n",
    "            \n",
    "# #             nn.Linear(1024, 128),\n",
    "# #             nn.ReLU(inplace=True),\n",
    "            \n",
    "# #             nn.Linear(128, 2)\n",
    "# #         )\n",
    "        \n",
    "# #     def forward_once(self, x):\n",
    "# #         # Forward pass \n",
    "# #         x = self.cnn1(x)\n",
    "# #         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "# #         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#     #     x = self.fc1(x)\n",
    "#     #     return x\n",
    "\n",
    "#     # def forward(self, input1, input2):\n",
    "#     #     # Forward pass of input 1\n",
    "#     #     output1 = self.forward_once(input1)\n",
    "#     #     # Forward pass of input 2\n",
    "#     #     output2 = self.forward_once(input2)\n",
    "#     #     return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallSiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SmallSiameseNetwork, self).__init__()\n",
    "#         # Setting up a smaller CNN\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=1),  # Fewer filters, smaller kernel size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Max pooling with smaller stride to reduce spatial dimensions\n",
    "#             nn.Dropout2d(p=0.2),  # Reduced dropout rate\n",
    "            \n",
    "#             nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),  # Fewer filters\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),  # Smaller max-pooling\n",
    "#             nn.Dropout2d(p=0.2),\n",
    "\n",
    "#             # nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.MaxPool2d(2, stride=2),\n",
    "#             # nn.Dropout2d(p=0.2),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to standardize output size (reduce to 3x3)\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))  # Smaller output size (3x3)\n",
    "        \n",
    "#         # Defining smaller fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(64 * 3 * 3, 128),  # Reduced size based on new feature map size\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.4),  # Keep some dropout for regularization\n",
    "            \n",
    "#             nn.Linear(128, 64),  # Smaller fully connected layer\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(64, 2)  # Output layer remains the same\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # Forward pass of input 1\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         # Forward pass of input 2\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        ### Binary:\n",
    "        # euclidian distance\n",
    "        # diff = x0 - x1\n",
    "        # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        # if torch.any(dist_sq < 0):\n",
    "        #     print('the value of dist_sq is negative: ' + dist_sq)\n",
    "        # # dist = torch.sqrt(torch.abs(dist_sq))\n",
    "        # dist = torch.sqrt(dist_sq + 1e-6)\n",
    "        # mdist = self.margin - dist\n",
    "        # dist = torch.clamp(mdist, min=0.0)\n",
    "        # loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        # loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "\n",
    "        label = y #binary?\n",
    "        euclidean_distance = nn.functional.pairwise_distance(x0, x1)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) + # similar\n",
    "                                (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)) # dissimilar\n",
    "        return loss_contrastive\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_network(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the appropriate device\n",
    "            # img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print statistics\n",
    "            if (i + 1) % 5 == 0:  # Print every 5 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if i == 5:\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [5/71], Loss: 0.8121\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_siamese_network(siamese_net, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(siamese_net.state_dict(), 'siamese_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "siamese_net.load_state_dict(torch.load('siamese_net.pth'))\n",
    "trained_model = siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist: tensor([0.2378]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Dist: tensor([0.6706]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Dist: tensor([1.1396]), Predicted: tensor([0.]), Actual: tensor([0.])\n",
      "Dist: tensor([0.8765]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Dist: tensor([1.1687]), Predicted: tensor([0.]), Actual: tensor([0.])\n",
      "Dist: tensor([0.4393]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Dist: tensor([1.5180]), Predicted: tensor([0.]), Actual: tensor([0.])\n",
      "Dist: tensor([0.5141]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Dist: tensor([1.0161]), Predicted: tensor([0.]), Actual: tensor([0.])\n",
      "Dist: tensor([1.3870]), Predicted: tensor([0.]), Actual: tensor([0.])\n",
      "Dist: tensor([0.8148]), Predicted: tensor([1.]), Actual: tensor([0.])\n",
      "Test Accuracy: 0.4545\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def test_siamese_network(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img1_path, img2_path, img1, img2, labels) in enumerate(test_loader):\n",
    "            # Move tensors to the appropriate device\n",
    "            # img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            # Calculate the euclidean distance between the outputs\n",
    "            dist = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Get predictions\n",
    "            predicted = (dist < 1.0).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predicted)\n",
    "\n",
    "            print(f\"Dist: {dist}, Predicted: {predicted}, Actual: {labels}\")\n",
    "\n",
    "            if i == 10:\n",
    "                break\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # confusion matrix:\n",
    "    matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    # plot:\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "test_siamese_network(trained_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3493e-17, -2.6962e-11]])\n",
      "tensor([[ 0.3319, -0.8129]])\n",
      "Dist: tensor([0.8781]), Predicted: tensor([1.]), Actual: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img1_path, img2_path, img1, img2, labels in eval_loader:\n",
    "        # Move tensors to the appropriate device\n",
    "        # img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output1, output2 = trained_model(img1, img2)\n",
    "        print(output1)\n",
    "        print(output2)\n",
    "\n",
    "        # Calculate the euclidean distance between the outputs\n",
    "        dist = F.pairwise_distance(output1, output2)\n",
    "\n",
    "        # Get predictions\n",
    "        predicted = (dist < 1.0).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_labels.extend(labels)\n",
    "        all_predictions.extend(predicted)\n",
    "\n",
    "        print(f\"Dist: {dist}, Predicted: {predicted}, Actual: {labels}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_test = eval_data.sample(1)\n",
    "# img1, img2, label = random_test.iloc[0]\n",
    "# img1 = img1.strip(\"()\")\n",
    "# img2 = img2.strip(\"()\")\n",
    "\n",
    "# # Load images\n",
    "# image1 = Image.open(img1).convert(\"RGB\")\n",
    "# image2 = Image.open(img2).convert(\"RGB\")\n",
    "\n",
    "# # Print Images\n",
    "# display(image1)\n",
    "# display(image2)\n",
    "\n",
    "# # Apply transformations\n",
    "# image1 = transform(image1).unsqueeze(0)\n",
    "# image2 = transform(image2).unsqueeze(0)\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = trained_model(image1, image2)\n",
    "\n",
    "# # Calculate the euclidean distance between the outputs\n",
    "# # diff = output1 - output2\n",
    "# # dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "# # dist = torch.sqrt(dist_sq)\n",
    "# dist = F.pairwise_distance(output1, output2)\n",
    "# print(f\"Distance: {dist.item()}\")\n",
    "\n",
    "# # Get predictions\n",
    "# predicted = (dist < 1.0).float()\n",
    "# true_label = 0 if label < 2 else 1\n",
    "\n",
    "# print(f\"Predicted: {predicted.item()}, Actual: {true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca of the output:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the output of the model\n",
    "outputs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img1, img2, label in eval_loader:\n",
    "        output1, output2 = trained_model(img1, img2)\n",
    "        outputs.append(output1)\n",
    "        labels.append(label)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_outputs = pca.fit_transform(outputs)\n",
    "\n",
    "# Plot the PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_outputs[:, 0], y=pca_outputs[:, 1], hue=labels, palette='viridis')\n",
    "plt.title('PCA of the Siamese Network Output')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pt\")\n",
    "# print(\"Model Saved Successfully\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_rows = full_data_paths[full_data_paths['similarity'].isna()].sample(5000)\n",
    "unlabeled_rows = unlabeled_rows[['image1_path', 'image2_path', 'similarity']]\n",
    "unlabeled_dataset = ImageSimilarityDataset(unlabeled_rows, transform=transform)\n",
    "data_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def active_learning_confident_samples(model, dataloader, margin=1.0, budget=100):\n",
    "    \"\"\"\n",
    "    Identify the least confident samples from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Siamese network.\n",
    "        dataloader: DataLoader for the dataset you want to evaluate.\n",
    "        margin: The margin used in the contrastive loss.\n",
    "        top_k: Number of least confident samples to return.\n",
    "    \n",
    "    Returns:\n",
    "        A list of the top_k least confident samples (input pairs and distances).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    least_confident_samples = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for img1_path, img2_path, img1, img2, labels in tqdm(dataloader):\n",
    "            # Get the model outputs for both images\n",
    "            output1, output2 = model(img1, img2)\n",
    "            \n",
    "            # Calculate pairwise distance\n",
    "            distances = F.pairwise_distance(output1, output2)\n",
    "            \n",
    "            # Calculate confidence score (distance from the margin)\n",
    "            confidence_scores = torch.abs(distances - margin)\n",
    "\n",
    "            # Collect the least confident samples (small confidence score means high uncertainty)\n",
    "            for i in range(len(confidence_scores)):\n",
    "                least_confident_samples.append((img1_path[i], img2_path[i], distances[i].item(), confidence_scores[i].item()))\n",
    "\n",
    "    # Sort samples by confidence score (ascending, to get least confident samples)\n",
    "    least_confident_samples.sort(key=lambda x: x[3])\n",
    "\n",
    "    # Return the top_k least confident samples\n",
    "    return least_confident_samples[:budget]\n",
    "\n",
    "least_confident_samples = active_learning_confident_samples(trained_model, data_loader, margin=1.0, budget=100)\n",
    "print(least_confident_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from confidence samples save into a csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
