{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate-Nearest-Neighbors Image Search Pipeline\n",
    "\n",
    "This notebook details a pipeline for performing approximate nearest neighbors search on a dataset of images by tokenizing the images into vectors using a pre-trained model. The Faiss library is then used for approximate nearest neighbors search. The metric used for the search is euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Dict, List\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_label</th>\n",
       "      <th>house_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350_ff091a99.jpg</td>\n",
       "      <td>farmhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258_560ea146.jpg</td>\n",
       "      <td>farmhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_d2c7428a.jpg</td>\n",
       "      <td>farmhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131_1251d4eb.jpg</td>\n",
       "      <td>farmhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447_96c74801.jpg</td>\n",
       "      <td>farmhouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_label house_type\n",
       "0  350_ff091a99.jpg  farmhouse\n",
       "1  258_560ea146.jpg  farmhouse\n",
       "2  001_d2c7428a.jpg  farmhouse\n",
       "3  131_1251d4eb.jpg  farmhouse\n",
       "4  447_96c74801.jpg  farmhouse"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the folder paths\n",
    "dataset_path = \"datasets/house_styles\"\n",
    "image_folder = \"datasets/house_styles/all_images\"\n",
    "\n",
    "sampled_images = pd.read_csv(os.path.join(dataset_path, \"sampled_labels.csv\"))\n",
    "sampled_image_names = sampled_images[\"file_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Query Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs:  101042\n",
      "Number of labeled pairs:  68093\n"
     ]
    }
   ],
   "source": [
    "# Paired data\n",
    "pairs_path = os.path.join(dataset_path, \"sampled_paired_labels_shuffled.csv\")\n",
    "pairs_df = pd.read_csv(pairs_path)\n",
    "\n",
    "# count labeled pairs:\n",
    "tagged_pairs = pairs_df[\"similarity\"].value_counts()\n",
    "print(\"Total number of pairs: \", len(pairs_df))\n",
    "print(\"Number of labeled pairs: \", tagged_pairs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of pair counts')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count pairs and build relevance judgments in a single pass\n",
    "pair_counts = defaultdict(int)\n",
    "relevance_judgments = defaultdict(dict)\n",
    "\n",
    "# Create temporary lookup array for better performance\n",
    "pairs_array = pairs_df[['image1', 'image2', 'similarity']].values\n",
    "\n",
    "for img1, img2, sim in pairs_array:\n",
    "    if sim in [0,1,2,3]:\n",
    "        # Update counts\n",
    "        pair_counts[img1] += 1\n",
    "        pair_counts[img2] += 1\n",
    "        \n",
    "        # Store relevance judgments (both directions)\n",
    "        relevance_judgments[img1][img2] = sim\n",
    "        relevance_judgments[img2][img1] = sim\n",
    "\n",
    "sorted_images_by_count = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(\"Images with the most tagged pairs:\")\n",
    "# for img, count in sorted_images_by_count[:50]:\n",
    "#     print(f\"{img}: {count}\")\n",
    "\n",
    "# distribution of pair counts:\n",
    "pair_counts_values = list(pair_counts.values())\n",
    "plt.hist(pair_counts_values, bins=12)\n",
    "plt.xlabel(\"Number of tagged pairs\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.title(\"Distribution of pair counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest pair count:  305\n"
     ]
    }
   ],
   "source": [
    "n_queries = 50\n",
    "queries = sorted_images_by_count[:n_queries]\n",
    "print(\"lowest pair count: \", queries[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the query images have at least 5 images labeled manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries:  50\n",
      "Number of index images:  400\n"
     ]
    }
   ],
   "source": [
    "query_image_names = [img for img, _ in queries]\n",
    "index_image_names = [img for img, _ in sorted_images_by_count[n_queries:]]\n",
    "\n",
    "print(\"Number of queries: \", len(query_image_names))\n",
    "print(\"Number of index images: \", len(index_image_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Computer Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained CLIP model and processor from Hugging Face\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set up the image transformation pipeline\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SiameseNetworkEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetworkEmbedder, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward_one(x)\n",
    "\n",
    "siamese_model = SiameseNetworkEmbedder()\n",
    "siamese_model.load_state_dict(torch.load(\"active_learning_models/net_round4.pth\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return clip_transform(image)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error loading image '{image_path}': {e}\")\n",
    "\n",
    "# Function to tokenize and get vector representation of an image\n",
    "def get_image_vector_clip(image_tensor):\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    inputs = clip_processor(images=image_tensor, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "def get_image_vector_siamese(image_tensor):\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    inputs = clip_processor(images=image_tensor, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    images_features = image_features.squeeze()\n",
    "\n",
    "    image_emb = siamese_model.get_embedding(images_features.to(device))\n",
    "    image_emb = image_emb.cpu().detach().numpy()\n",
    "    return image_emb\n",
    "\n",
    "def save_image_vectors(image_names, output_file_path):\n",
    "    clip_vectors_list = []\n",
    "    siamese_vectors_list = []\n",
    "\n",
    "    for image_name in tqdm(image_names):\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "\n",
    "        # try:\n",
    "        image_tensor = load_and_preprocess_image(image_path)\n",
    "        image_vector_clip = get_image_vector_clip(image_tensor)\n",
    "        image_vector_siamese = get_image_vector_siamese(image_tensor)\n",
    "\n",
    "        clip_vectors_list.append(image_vector_clip)\n",
    "        siamese_vectors_list.append(image_vector_siamese)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error processing image '{image_path}': {e}\")\n",
    "\n",
    "    clip_vectors = np.stack(clip_vectors_list)\n",
    "    siamese_vectors = np.stack(siamese_vectors_list)\n",
    "    \n",
    "    np.save(output_file_path + \"_clip.npy\", clip_vectors)\n",
    "    np.save(output_file_path + \"_siamese.npy\", siamese_vectors)\n",
    "    \n",
    "    return clip_vectors, siamese_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 clip image vectors saved, 50 siamese image vectors saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [08:13<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 clip image vectors saved, 400 siamese image vectors saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = \"vector_dbs/house_styles/\"\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(\"Output path already exists.\")\n",
    "else:\n",
    "    query_vectors_clip, query_vectors_siamese = save_image_vectors(query_image_names, output_path + \"vectors_query\")\n",
    "    print(f\"{len(query_vectors_clip)} clip image vectors saved, {len(query_vectors_siamese)} siamese image vectors saved.\")\n",
    "\n",
    "    index_vectors_clip, index_vectors_siamese = save_image_vectors(index_image_names, output_path + \"vectors_index\")\n",
    "    print(f\"{len(index_vectors_clip)} clip image vectors saved, {len(index_vectors_siamese)} siamese image vectors saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 693 index vectors and 174 query vectors.\n"
     ]
    }
   ],
   "source": [
    "output_path = \"vector_dbs/house_styles/\"\n",
    "\n",
    "index_vectors_clip = np.load(output_path + \"vectors_index_clip.npy\")\n",
    "query_vectors_clip = np.load(output_path + \"vectors_query_clip.npy\")\n",
    "index_vectors_siamese = np.load(output_path + \"vectors_index_siamese.npy\")\n",
    "query_vectors_siamese = np.load(output_path + \"vectors_query_siamese.npy\")\n",
    "\n",
    "print(f\"Loaded {len(index_vectors_clip)} index image vectors and {len(query_vectors_clip)} query image vectors.\")\n",
    "print(f\"Loaded {len(index_vectors_siamese)} index image vectors and {len(query_vectors_siamese)} query image vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def semi_optimized_exhaustive_search(\n",
    "#         index_vectors: np.ndarray,\n",
    "#         query_vectors: np.ndarray,\n",
    "#         k: int,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     This function performs an optimized exhaustive search.\n",
    "#     Args:\n",
    "#         index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "#         query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "#         dim: The dimensionality of the vectors.\n",
    "#     Returns:\n",
    "#         An array of shape (n_queries, k) containing the indices of the k nearest neighbors for each query vector.\n",
    "#     \"\"\"\n",
    "#     ann_lists = []\n",
    "#     for query_vec in tqdm(query_vectors):\n",
    "#         # distances = np.linalg.norm(index_vectors - query_vec, axis=1)\n",
    "#         distances = np.dot(index_vectors, query_vec)\n",
    "#         ann_lists.append(list(np.argsort(distances)[:k]))\n",
    "#     return np.array(ann_lists)\n",
    "\n",
    "def compute_recall_at_k(\n",
    "        nn_gt: np.ndarray,\n",
    "        ann: np.ndarray,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the recall@k.\n",
    "    Args:\n",
    "        nn_gt: The ground truth nearest neighbors.\n",
    "        ann: The approximate nearest neighbors.\n",
    "        k: The number of nearest neighbors to consider.\n",
    "    Returns:\n",
    "        The recall@k.\n",
    "    \"\"\"\n",
    "    return round(sum([len(set(ann[i]) & set(nn_gt[i])) / k for i in range(len(ann))])/len(ann), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_lsh_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "        nbits: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss LSH index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "        nbits: The number of bits to use in the hash.\n",
    "    Returns:\n",
    "        A Faiss LSH index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexLSH(dim, nbits)\n",
    "    index.add(index_vectors)\n",
    "    return index\n",
    "\n",
    "def build_faiss_flatl2_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss flat L2 index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "    Returns:\n",
    "        A Faiss flat L2 index.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    num_vectors = index_vectors.shape[0]\n",
    "    dim = index_vectors.shape[1]\n",
    "    norm_index_vectors = index_vectors / np.linalg.norm(index_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(norm_index_vectors)\n",
    "    return index\n",
    "\n",
    "def build_faiss_hnsw_index(\n",
    "        index_vectors: np.ndarray,\n",
    "        dim: int,\n",
    "        nlinks: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a Faiss HNSW index.\n",
    "    Args:\n",
    "        index_vectors: An array of shape (n_index, dim) containing the index vectors.\n",
    "        dim: The dimensionality of the vectors. \n",
    "        nlinks: The number of links to use in the graph.\n",
    "    Returns:\n",
    "        A Faiss HNSW index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexHNSWFlat(dim, nlinks, faiss.METRIC_L2)\n",
    "    index.add(index_vectors)\n",
    "    return index\n",
    "\n",
    "def faiss_search(\n",
    "        query_vectors: np.ndarray,\n",
    "        index: faiss.Index,\n",
    "        k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function uses a Faiss index to search for the k-nearest neighbors of query_vectors.\n",
    "    Args:\n",
    "        query_vectors: An array of shape (n_queries, dim) containing the query vectors. \n",
    "        index: A Faiss index.\n",
    "        k: The number of nearest neighbors to retrieve.\n",
    "    Returns:\n",
    "        An array of shape (, ) containing the indices of the k-nearest neighbors for each query vector.\n",
    "    \"\"\"\n",
    "    if not isinstance(index, faiss.Index):\n",
    "        raise ValueError(\"The index must be a Faiss index.\")\n",
    "    if isinstance(index, faiss.IndexFlatL2):\n",
    "        num_queries = query_vectors.shape[0]\n",
    "        dim = query_vectors.shape[1]\n",
    "        query_vectors = query_vectors / np.linalg.norm(query_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    distances, indices = index.search(query_vectors, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "dim = index_vectors_clip.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 18.4 ms, total: 32.4 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_lsh_index = build_faiss_lsh_index(index_vectors_clip, dim, nbits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 ms, sys: 1.78 ms, total: 2.8 ms\n",
      "Wall time: 4.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_l2_index = build_faiss_flatl2_index(index_vectors_clip, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 5.42 ms, total: 22.6 ms\n",
      "Wall time: 6.56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_hnsw_index = build_faiss_hnsw_index(index_vectors_clip, dim, nlinks=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Index Recall Compared to Exact Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 ms, sys: 2.49 ms, total: 4.72 ms\n",
      "Wall time: 1.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_l2_ann = faiss_search(query_vectors_clip, faiss_l2_index, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@10 for faiss_lsh_index: 0.09\n",
      "CPU times: user 2.17 ms, sys: 1.45 ms, total: 3.62 ms\n",
      "Wall time: 4.66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_lsh_ann = faiss_search(query_vectors_clip, faiss_lsh_index, k)\n",
    "\n",
    "print(f\"recall@10 for faiss_lsh_index: {compute_recall_at_k(faiss_l2_ann, faiss_lsh_ann, k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@10 for faiss_l2_index: 1.0\n",
      "CPU times: user 2.28 ms, sys: 3.25 ms, total: 5.53 ms\n",
      "Wall time: 3.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_hnsw_ann = faiss_search(query_vectors_clip, faiss_l2_index, k)\n",
    "\n",
    "print(f\"recall@10 for faiss_l2_index: {compute_recall_at_k(faiss_l2_ann, faiss_hnsw_ann, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "For vectors resulting from CLIP embeddings, we see the best performance in ANN compared to exact search in the -- Index. With a recall of --.\n",
    "\n",
    "We will use this index in the rest of out work and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMSLIB Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/naomi/miniconda3/envs/Lab2_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - nmslib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.8.30  |       hf0a4a13_0         155 KB  conda-forge\n",
      "    certifi-2024.7.4           |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    nmslib-2.1.1               |  py310hb6aeb05_0         592 KB\n",
      "    pybind11-2.13.5            |  py310h7306fd8_0         189 KB  conda-forge\n",
      "    pybind11-global-2.13.5     |  py310h7306fd8_0         177 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  nmslib             pkgs/main/osx-arm64::nmslib-2.1.1-py310hb6aeb05_0 \n",
      "  pybind11           conda-forge/osx-arm64::pybind11-2.13.5-py310h7306fd8_0 \n",
      "  pybind11-global    conda-forge/osx-arm64::pybind11-global-2.13.5-py310h7306fd8_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2024.7.2-h~ --> conda-forge::ca-certificates-2024.8.30-hf0a4a13_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.7.4~ --> conda-forge/noarch::certifi-2024.7.4-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "nmslib-2.1.1         | 592 KB    |                                       |   0% \n",
      "pybind11-2.13.5      | 189 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "pybind11-global-2.13 | 177 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.7.4     | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    | ##############################6       |  83% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2024 | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pybind11-2.13.5      | 189 KB    | ###1                                  |   8% \u001b[A\n",
      "\n",
      "nmslib-2.1.1         | 592 KB    | 9                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "pybind11-global-2.13 | 177 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "pybind11-2.13.5      | 189 KB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************"
     ]
    }
   ],
   "source": [
    "import nmslib\n",
    "\n",
    "# Example data\n",
    "data = np.random.random((1000, 128)).astype(np.float32)\n",
    "\n",
    "# Create an index using a custom distance function\n",
    "def custom_distance(vec1, vec2):\n",
    "    return np.sum(np.abs(vec1 - vec2))  # Example: Manhattan distance\n",
    "\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(data)\n",
    "index.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "# Query\n",
    "ids, distances = index.knnQuery(data[0], k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyNNDescent Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run this in Lab2_env:\n",
    "# conda install -c conda-forge pynndescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynndescent\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a unique distance function and compile it with Numba:\n",
    "@njit\n",
    "def custom_distance(x, y):\n",
    "    return np.sum(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pynndescent.NNDescent(index_vectors, metric=custom_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, distances = index.query(query_vectors, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:00<00:00, 4104.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive search time: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Exhaustive Search - Comparative Baseline ###\n",
    "start = time.time()\n",
    "gt_nn = semi_optimized_exhaustive_search(index_vectors, query_vectors, k)\n",
    "end = time.time()\n",
    "print(f\"Exhaustive search time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nndescent_index = pynndescent.NNDescent(index_vectors, metric='dot')\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query time: 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "indices, distances = nndescent_index.query(query_vectors, k)\n",
    "end = time.time()\n",
    "print(f\"Query time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20 for pynndescent: 0.02\n"
     ]
    }
   ],
   "source": [
    "recall = compute_recall_at_k(gt_nn, indices, k)\n",
    "print(f\"recall@{k} for pynndescent: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
