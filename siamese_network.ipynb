{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Similiarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of choosing the similarity function that determines the images that are similar to a given image.\n",
    "The implementation of the network is based on the Siamese network implementation. This class of networks is known to be more robust to class imbalance, so it fits the data on which we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <CDAC6E34-8608-3E70-8B2F-32BCD38E90FB> /Users/naomi/miniconda3/envs/Lab2_env/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         image1_path  \\\n",
      "0  (datasets/house_styles/all_images/001_d2c7428a...   \n",
      "1  (datasets/house_styles/all_images/453_d7b5d246...   \n",
      "2  (datasets/house_styles/all_images/116_32f01ef6...   \n",
      "3  (datasets/house_styles/all_images/301_b73b9663...   \n",
      "4  (datasets/house_styles/all_images/042_06b56791...   \n",
      "\n",
      "                                         image2_path  similarity  \n",
      "0  (datasets/house_styles/all_images/366_08eff319...         3.0  \n",
      "1  (datasets/house_styles/all_images/122_e44a0cb3...         0.0  \n",
      "2  (datasets/house_styles/all_images/174_55a7b3f9...         0.0  \n",
      "3  (datasets/house_styles/all_images/116_32f01ef6...         0.0  \n",
      "4  (datasets/house_styles/all_images/069_d3bedc1f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data_paths = pd.read_csv('datasets/house_styles/sampled_paired_labels_shuffled.csv')\n",
    "data_paths = data_paths[0:150][['image1_path', 'image2_path', 'similarity']]\n",
    "print(data_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (120, 3) Test:  (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "train_data = data_paths.sample(frac=0.8, random_state=42)\n",
    "test_data = data_paths.drop(train_data.index)\n",
    "\n",
    "print(\"Train: \", train_data.shape, \"Test: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSimilarityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.master_path = ''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images from the paths\n",
    "        image1_path = self.master_path + self.data.iloc[idx, 0].strip(\"()\")\n",
    "        image2_path = self.master_path + self.data.iloc[idx, 1].strip(\"()\")\n",
    "        \n",
    "        # Load images\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        \n",
    "        # Get similarity score\n",
    "        similarity = self.data.iloc[idx, 2]\n",
    "        \n",
    "        return image1, image2, torch.tensor(similarity, dtype=torch.float32)\n",
    "\n",
    "# Define transformations (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 (standard for many models)\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained models\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling layer to ensure the output size is consistent\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        x = self.cnn1(x)\n",
    "        x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # Forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Setting up the Sequential of CNN Layers\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=1),  # Adjusted for RGB input\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "#             nn.Dropout2d(p=0.3),\n",
    "#         )\n",
    "        \n",
    "#         # Adaptive pooling layer to ensure the output size is consistent\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))  # Adjust pooling size to handle dynamic input\n",
    "        \n",
    "#         # Defining the fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(256 * 6 * 6, 1024),  # Input size adjusted for adaptive pooling\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            \n",
    "#             nn.Linear(1024, 128),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward_once(self, x):\n",
    "#         # Forward pass \n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "    #     x = self.fc1(x)\n",
    "    #     return x\n",
    "\n",
    "    # def forward(self, input1, input2):\n",
    "    #     # Forward pass of input 1\n",
    "    #     output1 = self.forward_once(input1)\n",
    "    #     # Forward pass of input 2\n",
    "    #     output2 = self.forward_once(input2)\n",
    "    #     return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallSiameseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SmallSiameseNetwork, self).__init__()\n",
    "        # Setting up a smaller CNN\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=1),  # Fewer filters, smaller kernel size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # Max pooling with smaller stride to reduce spatial dimensions\n",
    "            nn.Dropout2d(p=0.2),  # Reduced dropout rate\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),  # Fewer filters\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # Smaller max-pooling\n",
    "            nn.Dropout2d(p=0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling layer to standardize output size (reduce to 3x3)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))  # Smaller output size (3x3)\n",
    "        \n",
    "        # Defining smaller fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * 3 * 3, 512),  # Reduced size based on new feature map size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),  # Keep some dropout for regularization\n",
    "            \n",
    "            nn.Linear(512, 64),  # Smaller fully connected layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(64, 2)  # Output layer remains the same\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        x = self.cnn1(x)\n",
    "        x = self.adaptive_pool(x)  # Adaptive pooling to standardize the feature map size\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # Forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        # euclidian distance\n",
    "        diff = x0 - x1\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "\n",
    "        mdist = self.margin - dist\n",
    "        dist = torch.clamp(mdist, min=0.0)\n",
    "\n",
    "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese_network(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (img1, img2, labels) in tqdm(enumerate(train_loader)):\n",
    "            # Move tensors to the appropriate device\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SmallSiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageSimilarityDataset(train_data, transform=transform)\n",
    "test_dataset = ImageSimilarityDataset(test_data, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_siamese_network(siamese_net, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pt\")\n",
    "# print(\"Model Saved Successfully\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
